{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import exists\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyxdf\n",
    "import mne\n",
    "from utils import *\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "print('Imports done...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Helper functions:\n",
    "def extract_eeg(stream, kick_last_ch=True):\n",
    "    \"\"\"\n",
    "    Extracts the EEG data and the EEG timestamp data from the stream and stores it into two lists.\n",
    "    :param stream: XDF stream containing the EEG data.\n",
    "    :param kick_last_ch: Boolean to kick out the brainproducts marker channel\n",
    "    :return: eeg: list containing the eeg data\n",
    "             eeg_ts: list containing the eeg timestamps.cd\n",
    "    \"\"\"\n",
    "    extr_eeg = stream['time_series'].T\n",
    "    extr_eeg *= 1e-6 # Convert to volts.\n",
    "    assert extr_eeg.shape[0] == 65\n",
    "    extr_eeg_ts = eeg_stream['time_stamps']\n",
    "\n",
    "    if kick_last_ch:\n",
    "        # Kick the last row (unused Brainproduct markers):\n",
    "        extr_eeg = extr_eeg[:64,:]\n",
    "\n",
    "    return extr_eeg, extr_eeg_ts\n",
    "\n",
    "\n",
    "def extract_eeg_infos(stream):\n",
    "    \"\"\"\n",
    "    Takes eeg stream and extracts the sampling rate, channel names, channel labels and the effective sample rate from the xdf info.\n",
    "    :param stream: EEG xdf stream\n",
    "    :return: sampling_rate: Configured sampling rate\n",
    "    :return: names: channel names\n",
    "    :return: labels: channel labels (eeg or eog)\n",
    "    :return: effective_sample_frequency: Actual sampling frequency based on timestamps.\n",
    "    \"\"\"\n",
    "    # Extract all infos from the EEG stream:\n",
    "    recording_device = stream['info']['name'][0]\n",
    "    sampling_rate = float(stream['info']['nominal_srate'][0])\n",
    "    effective_sample_frequency = float(stream['info']['effective_srate'])\n",
    "\n",
    "    # Extract channel names:\n",
    "    chn_names = [stream['info']['desc'][0]['channels'][0]['channel'][i]['label'][0] for i in range(64)]\n",
    "    # chn_names.append('Markers')\n",
    "    labels = ['eeg' for i in range(64)]\n",
    "    labels[16] = 'eog'\n",
    "    labels[21] = 'eog'\n",
    "    labels[40] = 'eog'\n",
    "    # chn_labels.append('misc')\n",
    "\n",
    "    return sampling_rate, chn_names, labels, effective_sample_frequency\n",
    "\n",
    "\n",
    "def extract_annotations(mark_stream, first_samp):\n",
    "    \"\"\"\n",
    "    Function to extract the triggers of the marker stream in order to prepare for the annotations.\n",
    "    :param mark_stream: xdf stream containing the markers and time_stamps\n",
    "    :param first_samp: First EEG sample, serves for aligning the markers\n",
    "    :return: triggs: Dict containing the extracted triggers.\n",
    "    \"\"\"\n",
    "    triggs = {'onsets': [], 'duration': [], 'description': []}\n",
    "\n",
    "    # Extract the markers:\n",
    "    marks = mark_stream['time_series']\n",
    "\n",
    "    # Fix markers due to bug in paradigm:\n",
    "    corrected_markers = fix_markers(marks)\n",
    "\n",
    "    # Extract the timestamp of the markers and correct them to zero\n",
    "    marks_ts = mark_stream['time_stamps'] - first_samp\n",
    "\n",
    "    # Read every trigger in the stream\n",
    "    for index, marker_data in enumerate(corrected_markers):\n",
    "        # extract triggers information\n",
    "        triggs['onsets'].append(marks_ts[index])\n",
    "        triggs['duration'].append(int(0))\n",
    "        # print(marker_data[0])\n",
    "        triggs['description'].append(marker_data[0])\n",
    "\n",
    "    return triggs\n",
    "\n",
    "# Fix markers:\n",
    "def fix_markers(orig_markers):\n",
    "    \"\"\"\n",
    "    Given a list of markers, this function processes the markers and modifies the trial type markers if necessary.\n",
    "\n",
    "    :param orig_markers: A list of markers. Each marker is a tuple containing the marker string and a float value representing the time at which the marker occurred.\n",
    "    :type orig_markers: list\n",
    "    :return: The modified list of markers.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "\n",
    "    trial_type_markers = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "    counter_letter = {'l': 'R', 'r': 'L', 'b': 'T', 't': 'B'}\n",
    "\n",
    "    # Parse through markers\n",
    "    for i in range(len(orig_markers)-3):\n",
    "        marker = orig_markers[i][0]\n",
    "        if marker in trial_type_markers:\n",
    "            following_markers = []\n",
    "            # Find the next 4 occurances that start with 'c':\n",
    "            # and store them in a list:\n",
    "            if (i+9) < len(orig_markers):\n",
    "                for ii in range(i+1, i+9):\n",
    "                    next_mark = orig_markers[ii][0]\n",
    "                    if next_mark[0] == 'c':\n",
    "                        following_markers.append(next_mark[2])\n",
    "            else:\n",
    "                for ii in range(i+1, len(orig_markers)):\n",
    "                    next_mark = orig_markers[ii][0]\n",
    "                    if next_mark[0] == 'c':\n",
    "                        following_markers.append(next_mark[2])\n",
    "\n",
    "            # Exit loop if less than 4 following markers were found:\n",
    "            if len(following_markers) < 4:\n",
    "                continue\n",
    "\n",
    "            if following_markers[0] == 'c' or following_markers[1] == 'c':\n",
    "                continue\n",
    "\n",
    "            # Extract first letter of the trial type marker:\n",
    "            first_letter = marker[0].lower()\n",
    "            last_letter = marker[-1].lower()\n",
    "\n",
    "            # Check if the first two letters in following markers are the same, if not, change type:\n",
    "            if (following_markers[0] != first_letter) and (following_markers[1] != first_letter):\n",
    "                # Trial type changes:\n",
    "                new_type = following_markers[0].upper() + 'T' + counter_letter[following_markers[0]] + '-'\n",
    "\n",
    "                if (following_markers[2] == 'c') and (following_markers[3] == 'c'):\n",
    "                    new_type = new_type + 's'\n",
    "                else:\n",
    "                    new_type = new_type + 'l'\n",
    "\n",
    "                orig_markers[i][0] = new_type\n",
    "\n",
    "            # Otherwise check if the second two markers are short or long and change accordingly:\n",
    "            else:\n",
    "                if (last_letter == 's') and (following_markers[2] != 'c') and (following_markers[3] != 'c'):\n",
    "                    new_type = marker[:-1]\n",
    "                    new_type += 'l'\n",
    "                    orig_markers[i][0] = new_type\n",
    "\n",
    "                elif (last_letter == 'l') and (following_markers[2] == 'c') and (following_markers[3] == 'c'):\n",
    "                    new_type = marker[:-1]\n",
    "                    new_type += 's'\n",
    "                    orig_markers[i][0] = new_type\n",
    "\n",
    "\n",
    "    return orig_markers\n",
    "\n",
    "def add_bad_channel_to_df(bad_chn_row, ch_names, csv_name='bad_channels.csv'):\n",
    "    \"\"\"\n",
    "    Add a row to a CSV file containing information about bad channels in some data.\n",
    "\n",
    "    :param bad_chn_row : list\n",
    "        A list containing the information to be added to the CSV file. The order of the elements should\n",
    "        match the order of the columns in the CSV file.\n",
    "    :param csv_name : str, optional\n",
    "        The name of the CSV file. The default is 'bad_channels.csv'.\n",
    "    :return: df_bads : pandas.DataFrame\n",
    "        A dataframe containing the information from the CSV file, with the new row added.\n",
    "    \"\"\"\n",
    "    # Check if df_bads.csv already exists:\n",
    "    if not exists(csv_name):\n",
    "        # Create dataframe with bad channels:\n",
    "        df_bads = pd.DataFrame(columns=['Subject', 'Run', 'Paradigm', 'Bad_channel'])\n",
    "        df_bads.to_csv(csv_name)\n",
    "    else:\n",
    "        # Load dataframe\n",
    "        df_bads = pd.read_csv(csv_name, index_col=0)\n",
    "\n",
    "    # Check if the channel name exists:\n",
    "    if bad_chn_row[-1] not in ch_names:\n",
    "        raise NameError('Channel name not found')\n",
    "\n",
    "    # Add row to the dataframe:\n",
    "    df_bads.loc[len(df_bads.index)] = bad_chn_row\n",
    "\n",
    "    print(f'Added {bad_chn_row} to the dataframe...')\n",
    "\n",
    "    # Drop duplicates:\n",
    "    df_bads.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Save df:\n",
    "    df_bads.to_csv(csv_name)\n",
    "\n",
    "    return df_bads\n",
    "\n",
    "def get_bads_for_subject(subject, csv_file='bad_channels.csv'):\n",
    "    \"\"\"\n",
    "    Get a list of bad channels that appear more than once for a given subject from a CSV file.\n",
    "\n",
    "    :param subject: Subject name.\n",
    "    :type subject: str\n",
    "    :param csv_file: CSV file containing bad channel information. Default is 'bad_channels.csv'.\n",
    "    :type: csv_file: str\n",
    "\n",
    "    :returns: list: List of bad channels that appear more than once.\n",
    "\n",
    "    :raises: FileExistsError: If the CSV file does not exist.\n",
    "    \"\"\"\n",
    "    # Check if df_bads.csv already exists:\n",
    "    if not exists(csv_file):\n",
    "        raise FileExistsError('File does not exist, please use the add_bad_channel_df() function.')\n",
    "    else:\n",
    "        # Load dataframe\n",
    "        df = pd.read_csv(csv_file, index_col=0)\n",
    "\n",
    "    # Filter for subject and check if channel has more then 1 appearances:\n",
    "    subject_df = df[df['Subject'] == subject]\n",
    "\n",
    "    # Get the counts of all the unique values in the 'column_name' column\n",
    "    channel_counts = subject_df['Bad_channel'].value_counts()\n",
    "\n",
    "    # Select the rows that have a count greater than 1\n",
    "    duplicate_bads = list(channel_counts[channel_counts>1].index)\n",
    "\n",
    "    return duplicate_bads\n",
    "\n",
    "def get_all_additional_information(subject, csv_file='participant_info.csv'):\n",
    "    \"\"\"Returns a tuple of additional information for the given subject.\n",
    "\n",
    "    :param subject: The name of the subject.\n",
    "    :type subject: str\n",
    "    :param csv_file: The file path to the participant info CSV file.\n",
    "    :type csv_file: str\n",
    "    :return: A tuple containing the following information:\n",
    "        - meas_date (datetime): The measurement date.\n",
    "        - experimenter (str): The name of the experimenter.\n",
    "        - proj_name (str): The name of the project.\n",
    "        - subject_info (str): The name of the subject.\n",
    "        - line_freq (float): The line frequency.\n",
    "        - gender (str): The gender of the subject.\n",
    "        - dob (str): The date of birth of the subject.\n",
    "        - age_at_meas (float): The age of the subject at the time of measurement.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    if not isinstance(subject, str):\n",
    "        raise TypeError('Subject must be a string.')\n",
    "    if not isinstance(csv_file, str):\n",
    "        raise TypeError('CSV file must be a string.')\n",
    "    if not exists(csv_file):\n",
    "        raise FileNotFoundError('File does not exist. Check if the path is correct.')\n",
    "\n",
    "    df = pd.read_csv(csv_file, index_col=False)\n",
    "    subject_info = df[df['Participant'] == subject]\n",
    "\n",
    "    if subject_info.empty:\n",
    "        raise ValueError('Subject not found in CSV file.')\n",
    "\n",
    "    meas_date_str = subject_info['Measurement_Date'].values[0]\n",
    "    meas_date = datetime.strptime(meas_date_str, '%d.%m.%Y')\n",
    "    meas_date = meas_date.replace(tzinfo=timezone.utc)\n",
    "    experimenter = 'Peter T.'\n",
    "    proj_name = 'Decoding of range during goal-directed movement'\n",
    "    line_freq = 50.0\n",
    "    gender = subject_info['Gender'].values[0]\n",
    "    dob = subject_info['Date_Of_Birth'].values[0]\n",
    "    age_at_meas = subject_info['Age_At_Measurement'].values[0]\n",
    "\n",
    "    return meas_date, experimenter, proj_name, subject_info, line_freq, gender, dob, age_at_meas\n",
    "\n",
    "def get_subset_of_dict(full_dict, keys_of_interest):\n",
    "    return dict((k, full_dict[k]) for k in keys_of_interest if k in full_dict)\n",
    "\n",
    "\n",
    "def create_sliced_trial_list(event_dict, events_from_annot):\n",
    "    # Slice into list of list from trial_type_marker to trial_type_marker\n",
    "    trial_type_markers = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "    event_dict_trial_type = get_subset_of_dict(event_dict, trial_type_markers)\n",
    "    event_sequence = events_from_annot[:,-1]\n",
    "\n",
    "    trial_list = []\n",
    "    first_samps = []\n",
    "    first_time = True\n",
    "    for i, entry in enumerate(event_sequence):\n",
    "        if entry in event_dict_trial_type.values():\n",
    "            if first_time:\n",
    "                temp_list = [entry]\n",
    "                first_samps.append(events_from_annot[i,0])\n",
    "                first_time = False\n",
    "            else:\n",
    "                temp_list.append(entry)\n",
    "                trial_list.append(temp_list)\n",
    "                temp_list = [entry]\n",
    "                first_samps.append(events_from_annot[i,0])\n",
    "        else:\n",
    "            if not first_time:\n",
    "                temp_list.append(entry)\n",
    "\n",
    "    trial_list.append(temp_list)\n",
    "\n",
    "    return trial_list, first_samps\n",
    "\n",
    "\n",
    "def get_bad_epochs(event_dict, trial_list):\n",
    "    \"\"\"\n",
    "    Given an event dictionary, find the indices of the epochs (sub-lists) in the trial list that are invalid.\n",
    "    An epoch is invalid if it does not satisfy the following conditions:\n",
    "        1. If it is not the last epoch, its length must be 9.\n",
    "        2. If it is the last epoch, its length must be 8.\n",
    "        3. The first entry must be a trial_type marker.\n",
    "        4. The second entry must be the 'Start' marker.\n",
    "        5. The fourth entry must be the 'Cue' marker.\n",
    "        6. The seventh entry must be the 'Break' marker.\n",
    "        7. The first two LDR readings must be coherent with the trial type.\n",
    "        8. The second two LDR readings must be coherent with the trial type.\n",
    "\n",
    "    :param event_dict: A dictionary where keys are event names and values are corresponding event markers.\n",
    "    :type event_dict: dict\n",
    "    :return: A list of indices corresponding to the invalid epochs.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the order is correct:\n",
    "    bad_idcs = []\n",
    "    trial_type_markers = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "    trial_vals = [event_dict[key] for key in trial_type_markers]\n",
    "    n_epochs = len(trial_list)\n",
    "\n",
    "    for idx, sub_list in enumerate(trial_list):\n",
    "        # Add bad epoch if the length is not 9 (except for the last epoch):\n",
    "        if len(sub_list) != 9 and idx != n_epochs-1:\n",
    "            bad_idcs.append(idx)\n",
    "            continue\n",
    "\n",
    "        # Add bad epoch if the length is not 8 for the last epoch:\n",
    "        elif len(sub_list) != 8 and idx == n_epochs-1:\n",
    "            bad_idcs.append(idx)\n",
    "            continue\n",
    "\n",
    "        # Add bad epoch if the first entry is not a trial_type_marker:\n",
    "        if sub_list[0] not in trial_vals:\n",
    "            bad_idcs.append(idx)\n",
    "            continue\n",
    "\n",
    "        # Add bad epoch if the second entry is not a Start marker:\n",
    "        if sub_list[1] != event_dict['Start']:\n",
    "            bad_idcs.append(idx)\n",
    "            continue\n",
    "\n",
    "        # Add bad epoch if the fourth entry is not a Cue marker:\n",
    "        if sub_list[3] != event_dict['Cue']:\n",
    "            bad_idcs.append(idx)\n",
    "            continue\n",
    "\n",
    "        # Add bad epoch if the seventh entry is not a Break marker:\n",
    "        if sub_list[6] != event_dict['Break']:\n",
    "            bad_idcs.append(idx)\n",
    "            continue\n",
    "\n",
    "        # Get the keys for entries 3,5,6 and 8:\n",
    "        start_touch = list(event_dict.keys())[list(event_dict.values()).index(sub_list[2])]\n",
    "        start_release = list(event_dict.keys())[list(event_dict.values()).index(sub_list[4])]\n",
    "        target_touch = list(event_dict.keys())[list(event_dict.values()).index(sub_list[5])]\n",
    "        target_release = list(event_dict.keys())[list(event_dict.values()).index(sub_list[7])]\n",
    "\n",
    "        # Get key for the trial_type marker:\n",
    "        trial_type = list(event_dict.keys())[list(event_dict.values()).index(sub_list[0])]\n",
    "\n",
    "        # Add bad epoch if first two ldr readings are not coherent with the trial type:\n",
    "        if (trial_type[0].lower() != start_touch[2]) or (trial_type[0].lower() != start_release[2]):\n",
    "            bad_idcs.append(idx)\n",
    "            continue\n",
    "\n",
    "        # Add bad epoch if the second two ldr readings are not coherent with the second part of the trial type:\n",
    "        if (trial_type[4] == 'l'):\n",
    "            if (trial_type[2].lower() != target_touch[2]) or (trial_type[2].lower() != target_release[2]):\n",
    "                bad_idcs.append(idx)\n",
    "                continue\n",
    "\n",
    "        if (trial_type[4] == 's'):\n",
    "            if (target_touch[2] != 'c') or (target_release[2] != 'c'):\n",
    "                bad_idcs.append(idx)\n",
    "                continue\n",
    "\n",
    "    return bad_idcs\n",
    "\n",
    "def convert_samps_to_time(first_time, first_samp, samp_list):\n",
    "    \"\"\"Convert sample numbers to time values.\n",
    "    :param first_time: float time value of the first sample\n",
    "    :param first_samp: int sample number of the first sample\n",
    "    :param samp_list: list of int sample numbers to be converted\n",
    "    :return: numpy ndarray of time values for the input sample numbers\n",
    "    \"\"\"\n",
    "    return np.array(samp_list) * first_time / first_samp\n",
    "\n",
    "def create_bad_annotations(starting_times, bad_events, duration, orig_time):\n",
    "    \"\"\"Create annotations for bad events in EEG data.\n",
    "\n",
    "    :param starting_times: 1D array of starting times for all events in EEG data\n",
    "    :type starting_times: numpy.ndarray\n",
    "    :param bad_events: Indices of bad events in the starting_times array\n",
    "    :type bad_events: numpy.ndarray or list\n",
    "    :param duration: Duration of the bad events\n",
    "    :type duration: float\n",
    "    :param orig_time: The time at which the first sample in data was recorded\n",
    "    :type orig_time: float\n",
    "    :return: mne.Annotations object containing onsets, durations, and descriptions for bad events\n",
    "    :rtype: mne.Annotations\n",
    "    \"\"\"\n",
    "\n",
    "    bad_times = starting_times[bad_events]\n",
    "    onsets = bad_times\n",
    "    durations = [duration] * len(bad_times)\n",
    "    descriptions = ['bad epoch'] * len(bad_times)\n",
    "    return mne.Annotations(onsets, durations, descriptions, orig_time=orig_time)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Constants"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data_path = 'C:/Users/tumfart/Code/github/master-thesis/data/'\n",
    "data_path = 'C:/Users/peter/Google Drive/measurements/eeg/'\n",
    "subjects = ['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07' , 'A08', 'A09', 'A10']\n",
    "# = 'A03'\n",
    "paradigm = 'paradigm' # 'eye', 'paradigm'\n",
    "plot = False\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "trial_type_markers = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "\n",
    "# Create path list for each subject:\n",
    "paths = [str(data_path + subject + '/' + paradigm) for subject in subjects]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read xdf-files for specified subject"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Extracting subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '.xdf' in f]\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "        file = path + '/' + file_name\n",
    "\n",
    "        # Read the raw stream:\n",
    "        streams, header = pyxdf.load_xdf(file)\n",
    "\n",
    "        # Split the streams:\n",
    "        eeg_stream, marker_stream = split_streams(streams)\n",
    "\n",
    "        # Get the eeg data:\n",
    "        eeg, eeg_ts = extract_eeg(eeg_stream, kick_last_ch=True)\n",
    "        #max_eeg_ts.append(eeg_ts.max())\n",
    "\n",
    "        # Extract all infos from the EEG stream:\n",
    "        fs, ch_names, ch_labels, eff_fs = extract_eeg_infos(eeg_stream)\n",
    "\n",
    "        # Extract the triggers from the marker stream:\n",
    "        triggers = extract_annotations(marker_stream, first_samp=eeg_ts[0])\n",
    "\n",
    "        # Define MNE annotations\n",
    "        annotations = mne.Annotations(triggers['onsets'], triggers['duration'], triggers['description'], orig_time=None)\n",
    "\n",
    "        # Create mne info:\n",
    "        # TODO: Check what info can be added to the stream:\n",
    "        info = mne.create_info(ch_names, fs, ch_labels)\n",
    "\n",
    "        # Create the raw array and add info, montage and annotations:\n",
    "        raw = mne.io.RawArray(eeg, info, first_samp=eeg_ts[0])\n",
    "        raw.set_montage('standard_1005')\n",
    "        raw.set_annotations(annotations)\n",
    "\n",
    "        # Store the raw file:\n",
    "        store_name = path + '/' + subject + '_run_' + str(i + 1) + '_unprocessed_raw.fif'\n",
    "        raw.save(store_name, overwrite=True)\n",
    "\n",
    "        if plot:\n",
    "            raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                     remove_dc=False, title='Raw')\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished reading, took me {round(time.time()-start)} seconds...')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Concatenate all raws for a subject:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and load the raw files:\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading raw files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_unprocessed_raw.fif' in f]\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "        if plot:\n",
    "            raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                     remove_dc=False, title='Highpass filtered')\n",
    "            plot_spectrum(raw)\n",
    "\n",
    "\n",
    "        # Highpass filter:\n",
    "        raw_highpass = raw.copy().filter(l_freq=0.4, h_freq=None, picks=['eeg'], method='iir')\n",
    "        if plot:\n",
    "            raw_highpass.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                              remove_dc=False, title='Highpass filtered')\n",
    "            plot_spectrum(raw_highpass)\n",
    "\n",
    "        # Notch filter:\n",
    "        raw_notch = raw_highpass.copy().notch_filter(freqs=[50], picks=['eeg'])\n",
    "        if plot:\n",
    "            raw_notch.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title='Notch filtered')\n",
    "            plot_spectrum(raw_notch)\n",
    "\n",
    "        # Store the raw file:\n",
    "        store_name = path + '/' + subject + '_run_' + str(i + 1) + '_highpass_notch_filtered_raw.fif'\n",
    "        raw_notch.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished highpass and notch filtering, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter the signals"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and load the raw files:\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading raw files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_unprocessed_raw.fif' in f]\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "        if plot:\n",
    "            raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                              remove_dc=False, title='Highpass filtered')\n",
    "            plot_spectrum(raw)\n",
    "\n",
    "\n",
    "        # Highpass filter:\n",
    "        raw_highpass = raw.copy().filter(l_freq=0.4, h_freq=None, picks=['eeg'], method='iir')\n",
    "        if plot:\n",
    "            raw_highpass.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                              remove_dc=False, title='Highpass filtered')\n",
    "            plot_spectrum(raw_highpass)\n",
    "\n",
    "        # Notch filter:\n",
    "        raw_notch = raw_highpass.copy().notch_filter(freqs=[50], picks=['eeg'])\n",
    "        if plot:\n",
    "            raw_notch.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title='Notch filtered')\n",
    "            plot_spectrum(raw_notch)\n",
    "\n",
    "        # Store the raw file:\n",
    "        store_name = path + '/' + subject + '_run_' + str(i + 1) + '_highpass_notch_filtered_raw.fif'\n",
    "        raw_notch.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished highpass and notch filtering, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize signals for bad channel identification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specifiy subject:\n",
    "subject = 'A01'\n",
    "paradigm = 'paradigm'\n",
    "if paradigm == 'paradigm':\n",
    "    runs = 9\n",
    "else:\n",
    "    runs = 2\n",
    "names = [subject + '_run_' + str(i + 1) + '_highpass_notch_filtered_raw.fif' for i in range(runs)]\n",
    "\n",
    "for i, name in enumerate(names):\n",
    "    file = data_path + subject + '/' + paradigm + '/' + name\n",
    "    raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "    raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title=f'Notch & HP filtered. Run: {i+1}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specifiy subject:\n",
    "subject = 'A10'\n",
    "paradigm = 'eye'\n",
    "run = 2\n",
    "name = subject + '_run_' + str(run) + '_highpass_notch_filtered_raw.fif'\n",
    "file = data_path + subject + '/' + paradigm + '/' + name\n",
    "\n",
    "raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title=f'Notch & HP filtered. Run: {run}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add bad channel to bad channel.csv:\n",
    "bad_df = add_bad_channel_to_df([subject, run, paradigm, 'T8'], ch_names=raw.ch_names, csv_name='bad_channels.csv')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add bad channels to all raw infos:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if 'raw.fif' in f]\n",
    "    # Add bad channels:\n",
    "    bads = get_bads_for_subject(subject, csv_file='bad_channels.csv')\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "        raw.info['bads'] = bads\n",
    "\n",
    "        # Overwrite the raw file with the added info:\n",
    "        store_name = path + '/' + file_name\n",
    "        raw.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished bad channel adding, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Perform interpolation of bad channels:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_highpass_notch_filtered_raw.fif' in f]\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "        # Interpolate bad channels:\n",
    "        raw_interp = raw.copy().interpolate_bads(reset_bads=False)\n",
    "\n",
    "        # Overwrite the raw file with the added info:\n",
    "        store_name = path + '/' + subject + '_run_' + str(i + 1) + '_bad_channels_interpolated_raw.fif'\n",
    "        raw_interp.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished interpolating bad channels, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CAR re-referencing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "\n",
    "    #TODO: Loaded fif file changes in final pipeline (because eye artifact correction was not yet implemented).\n",
    "    file_names = [f for f in listdir(path) if '_bad_channels_interpolated_raw.fif' in f]\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw_interp = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "        # Interpolate bad channels:\n",
    "        raw_avg_ref = raw_interp.copy().set_eeg_reference(ref_channels='average')\n",
    "\n",
    "        # Overwrite the raw file with the added info:\n",
    "        store_name = path + '/' + subject + '_run_' + str(i + 1) + '_car_referenced_raw.fif'\n",
    "        raw_avg_ref.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished rereferencing eeg, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load uninterpolated raw and interpolated raw:\n",
    "raw_avg_ref = mne.io.read_raw('C:/Users/peter/Google Drive/measurements/eeg/A01/paradigm/A01_run_1_car_referenced_raw.fif')\n",
    "raw_interp = mne.io.read_raw('C:/Users/peter/Google Drive/measurements/eeg/A01/paradigm/A01_run_1_bad_channels_interpolated_raw.fif')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_avg_ref.plot(duration=60, proj=False, n_channels=len(raw_avg_ref.ch_names), remove_dc=False, title=f'CAR referenced.')\n",
    "raw_interp.plot(duration=60, proj=False, n_channels=len(raw_interp.ch_names), remove_dc=False, title='Interpolated')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper cell to add info:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if 'raw.fif' in f]\n",
    "\n",
    "    # Get correct info:\n",
    "    meas_date, experimenter, proj_name, subject_info, line_freq, gender, dob, age_at_meas = get_all_additional_information(subject, csv_file='participant_info.csv')\n",
    "\n",
    "    big_subject_info = {'Subject ID': subject,\n",
    "                        'Gender': gender,\n",
    "                        'Age at measurement': age_at_meas}\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "        # Add infos:\n",
    "        raw.info['subject_info'] = big_subject_info\n",
    "        raw.info['experimenter'] = experimenter\n",
    "        #raw.info['proj_name'] = proj_name\n",
    "        raw.set_meas_date(meas_date)\n",
    "        raw.info['line_freq'] = line_freq\n",
    "\n",
    "        # Overwrite the raw file with the added info:\n",
    "        store_name = path + '/' + file_name\n",
    "        raw.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished adding info, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HEAR - High-variance electrode artifact removal algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Implement HEAR\n",
    "# Get resting data:\n",
    "\n",
    "# Check resting trials and exclude bad ones:\n",
    "\n",
    "# Calculate variance µ^2_s\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lowpass filter at 3 Hz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and load the raw files:\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading raw files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_car_referenced_raw.fif' in f]\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "        # Lowpass filter:\n",
    "        raw_lowpass = raw.copy().filter(l_freq=None, h_freq=3.0, picks=['eeg'], method='iir')\n",
    "        if plot:\n",
    "            raw_lowpass.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                              remove_dc=False, title='Highpass filtered')\n",
    "            plot_spectrum(raw_lowpass)\n",
    "\n",
    "        # Store the raw file:\n",
    "        store_name = path + '/' + subject + '_run_' + str(i + 1) + '_lowpass_filtered_raw.fif'\n",
    "        raw_lowpass.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished lowpass filtering, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combine the datasets into one dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_lowpass_filtered_raw.fif' in f]\n",
    "\n",
    "    raws = []\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "        raws.append(raw)\n",
    "\n",
    "    concat_raw = mne.concatenate_raws(raws)\n",
    "\n",
    "    # Store the concatenated raw file:\n",
    "    store_name = path + '/' + subject + '_' + paradigm + '_concatenated_raw.fif'\n",
    "    concat_raw.save(store_name, overwrite=True)\n",
    "    print()\n",
    "\n",
    "print(f'Finished concatenating, took me {round(time.time() - start)} seconds...')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "concat_raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title=f'Concatenated raw.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mark bad dataspans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if 'concatenated_raw.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "    events_from_annot, event_dict = mne.events_from_annotations(raw)\n",
    "\n",
    "\n",
    "    # Select subset of event_dict with following markers:\n",
    "    markers_of_interest = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "    event_dict_of_interest = get_subset_of_dict(event_dict, markers_of_interest)\n",
    "\n",
    "    # Check if the order of annotations is correct:\n",
    "    # Therefore first create a marker list of each trial:\n",
    "    trial_list, starting_samples = create_sliced_trial_list(event_dict, events_from_annot)\n",
    "    starting_times = convert_samps_to_time(raw.first_time, raw.first_samp, starting_samples)\n",
    "    bad_events = get_bad_epochs(event_dict, trial_list)\n",
    "    print(len(bad_events))\n",
    "\n",
    "    # add annotation for bad channels and select reject_by_annotation when generating the epochs:\n",
    "    bad_annots = create_bad_annotations(starting_times, bad_events, duration=7, orig_time=raw.info['meas_date'])\n",
    "    raw.set_annotations(raw.annotations + bad_annots)\n",
    "\n",
    "    # Save epochs:\n",
    "    store_name = path + '/' + subject + '_' + paradigm + '_bad_annotations_raw.fif'\n",
    "    raw.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished adding bad annotations, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Epoching"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_bad_annotations_raw.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "    events_from_annot, event_dict = mne.events_from_annotations(raw)\n",
    "\n",
    "\n",
    "    # Select subset of event_dict with following markers:\n",
    "    markers_of_interest = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "    event_dict_of_interest = get_subset_of_dict(event_dict, markers_of_interest)\n",
    "\n",
    "    # TODO select event ID's of interest, hand over dict for event_id to make it easier to extract them:\n",
    "    epochs = mne.Epochs(raw, events_from_annot, event_id=event_dict_of_interest, tmin=2.0, tmax=7.0, baseline=None, reject_by_annotation=True, preload=True)\n",
    "\n",
    "    # Save epochs:\n",
    "    store_name = path + '/' + subject + '_' + paradigm + '_epo.fif'\n",
    "    epochs.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished epoching, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "help(create_bad_annotations)\n",
    "epochs = epochs.copy().resample(10)\n",
    "epochs.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evokeds_list = [epochs['LTR-l', 'RTL-l', 'TTB-l', 'BTT-l'].average(), epochs['LTR-s', 'RTL-s', 'TTB-s', 'BTT-s'].average()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conds = ('long', 'short')\n",
    "evks = dict(zip(conds, evokeds_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evokeds2 = dict(short=list(epochs['LTR-l', 'RTL-l', 'TTB-l', 'BTT-l'].iter_evoked()),\n",
    "                long=list(epochs['LTR-s', 'RTL-s', 'TTB-s', 'BTT-s'].iter_evoked()))\n",
    "mne.viz.plot_compare_evokeds(evokeds2, combine='mean', picks=['Cz', 'C1', 'C2', 'FCz', 'CPz'], show_sensors='upper right')\n",
    "#picks=['Cz', 'C1', 'C2', 'FCz', 'CPz']\n",
    "# plt.savefig('distance_grand_averages.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subjects = ['A05']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mne.viz.plot_compare_evokeds(evks, picks='Fcz')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp = epochs['LTR-l', 'RTL-l']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def custom_func(x):\n",
    "    return x.max(axis=1)\n",
    "\n",
    "\n",
    "for combine in ('mean', 'median', 'gfp', custom_func):\n",
    "    mne.viz.plot_compare_evokeds(evks, picks='eeg', combine=combine)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "event_dict['BTT-s']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs['BTT-l'].plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read xdf:\n",
    "# Read the raw stream:\n",
    "streams, header = pyxdf.load_xdf('C:/Users/peter/Google Drive/measurements/eeg/A02/paradigm/sub-A02_ses-S001_task-Paradigm[_acq-]_run-002_eeg.xdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper cell to add bad epochs to a dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: cell to view the epochs for a specific subject and marker:\n",
    "marker_of_interest = 'LTR-s' # ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "subject = 'A03'\n",
    "file = data_path + subject + '/paradigm/' + subject + '_paradigm_epo.fif'\n",
    "\n",
    "# Load epochs:\n",
    "epochs = mne.read_epochs(file, preload=True)\n",
    "\n",
    "epochs[marker_of_interest].plot()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp = epochs[marker_of_interest][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "replace_list = [list(event_dict.keys())[list(event_dict.values()).index(events_from_annot[i,2])] for i in range(len(events_from_annot))]\n",
    "\n",
    "events_from_annot[:,2] = replace_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(len(events_from_annot)):\n",
    "    events_from_annot[i,2] = list(event_dict.keys())[list(event_dict.values()).index()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get metrics of rejected channels per subject"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "\n",
    "num_bads = []\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '.fif' in f]\n",
    "\n",
    "    # Load one .fif file:\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "    bads = raw.info['bads']\n",
    "    num_bads.append(len(bads))\n",
    "    print()\n",
    "\n",
    "num_bads = np.asarray(num_bads)\n",
    "print(f'Rejceted on average {num_bads.mean()} +/- {round(num_bads.std(),2)}')\n",
    "\n",
    "print(f'Finished calculating rejected channel metrics, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List files in folder:\n",
    "files = [f for f in listdir(path)]\n",
    "\n",
    "eeg_streams = []\n",
    "marker_streams = []\n",
    "# Load all recorded EEG files for one subjectc\n",
    "files = [files[0]]\n",
    "for file in files:\n",
    "    file_name = path + '/' + file\n",
    "    print(f'####', end='#')\n",
    "\n",
    "    # Read streams\n",
    "    streams, header = pyxdf.load_xdf(file_name)\n",
    "\n",
    "    # Split the streams:\n",
    "    eeg_stream, marker_stream = split_streams(streams)\n",
    "\n",
    "    eeg_streams.append(eeg_stream)\n",
    "    marker_streams.append(marker_stream)\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Finished reading, found {len(eeg_streams)} EEG streams and {len(marker_streams)} marker streams...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "differences = [0]\n",
    "max_eeg_ts = []\n",
    "for i, (eeg_stream, m_stream) in enumerate(zip(eeg_streams, marker_streams)):\n",
    "    # Get the eeg data:\n",
    "    eeg, eeg_ts = extract_eeg(eeg_stream)\n",
    "    max_eeg_ts.append(eeg_ts.max())\n",
    "\n",
    "    # Kick the last row (unused Brainproduct markers):\n",
    "    eeg = eeg[:64,:]\n",
    "\n",
    "    # Extract all infos from the EEG stream:\n",
    "    fs, ch_names, ch_labels, eff_fs = extract_eeg_infos(eeg_stream)\n",
    "\n",
    "    # Extract the markers and timestamps:\n",
    "    # markers = m_stream['time_series']\n",
    "    # markers_ts = m_stream['time_stamps']\n",
    "    #\n",
    "    # # Convert list of list of strings to list of strings:\n",
    "    # markers = [''.join(element) for element in markers]\n",
    "\n",
    "    # # Make Nan array with len(eeg)\n",
    "    # aligned_markers = np.empty(eeg_ts.shape, dtype='<U5')\n",
    "    #\n",
    "    # # Place markers string at the align array where first time markers_ts <= eeg_ts:\n",
    "    # for k, marker in enumerate(markers):\n",
    "    #     ts = markers_ts[k]\n",
    "    #     idx = np.where(ts <= eeg_ts)[0][0]\n",
    "    #     aligned_markers[idx] = marker\n",
    "\n",
    "    if i == 0:\n",
    "        global_eeg = eeg\n",
    "        first_ts = eeg_ts[0]\n",
    "        # global_markers = aligned_markers\n",
    "    else:\n",
    "        global_eeg = np.concatenate((global_eeg, eeg), axis=1)\n",
    "        # global_markers = np.concatenate((global_markers, aligned_markers))\n",
    "        differences.append(eeg_ts[0]-last_ts)\n",
    "\n",
    "    last_ts = eeg_ts[-1]\n",
    "    print(f'####', end='#')\n",
    "\n",
    "cum_diff = np.cumsum(differences)\n",
    "eeg = global_eeg\n",
    "# markers = global_markers\n",
    "print()\n",
    "print('Extracted EEG data, EEG infos...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# annotation generation from:\n",
    "# https://github.com/WriessneggerLab/EEG-preprocessing/blob/eeg/src/EEGAnalysis.py\n",
    "# generation of the events according to the definition\n",
    "triggers = {'onsets': [], 'duration': [], 'description': []}\n",
    "global_markers_ts = []\n",
    "for i, m_stream in enumerate(marker_streams):\n",
    "    # Extract the markers and timestamps:\n",
    "    markers = m_stream['time_series']\n",
    "    markers_ts = m_stream['time_stamps'] - float(m_stream['info']['created_at'][0])# - cum_diff[i]\n",
    "\n",
    "\n",
    "    global_markers_ts += list(markers_ts)\n",
    "    # read every trigger in the stream\n",
    "    for idx, marker_data in enumerate(markers):\n",
    "        # extract triggers information\n",
    "        triggers['onsets'].append(markers_ts[idx])\n",
    "        triggers['duration'].append(int(0))\n",
    "        # print(marker_data[0])\n",
    "        triggers['description'].append(marker_data[0])\n",
    "\n",
    "# define MNE annotations\n",
    "annotations = mne.Annotations(triggers['onsets'], triggers['duration'], triggers['description'], orig_time=None) #, orig_time=np.array(global_markers_ts))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mrks_list = list(markers_ts)\n",
    "a = []\n",
    "a += mrks_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Put extracted data into mne structure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: align annotations\n",
    "\n",
    "info = mne.create_info(ch_names, fs, ch_labels)\n",
    "\n",
    "raw = mne.io.RawArray(eeg, info, first_samp=first_ts)\n",
    "raw.set_montage('standard_1005')\n",
    "raw.set_annotations(annotations)\n",
    "\n",
    "if plot:\n",
    "    raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "             remove_dc=False, title='Raw')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter with HP at 0.4Hz and BS at 50 Hz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_highpass = raw.copy().filter(l_freq=0.4, h_freq=None, picks=['eeg'], method='iir')\n",
    "if plot:\n",
    "    raw_highpass.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                      remove_dc=False, title='Highpass filtered')\n",
    "    plot_spectrum(raw_highpass)\n",
    "\n",
    "raw_notch = raw_highpass.copy().notch_filter(freqs=[50], picks=['eeg'])\n",
    "if plot:\n",
    "    raw_notch.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title='Notch filtered')\n",
    "    plot_spectrum(raw_notch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Interpolate bad channels:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: check function --> need to mark them first\n",
    "raw_interp = raw_notch.copy().interpolate_bads(reset_bads=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correct eye artifacts:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CAR:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_avg_ref = raw_interp.copy().set_eeg_reference(ref_channels='average')\n",
    "if plot:\n",
    "    raw_avg_ref.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title='CAR Referenced')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HEAR model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LP at 3.0Hz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_lp = raw_avg_ref.copy().filter(l_freq=None, h_freq=3.0, picks=['eeg'], method='iir')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract epochs before resampling (otherwise markers may get lost) and reject bad trials:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "events = mne.find_events(raw_lp, stim_channel='Markers')\n",
    "\n",
    "epochs = mne.Epochs(raw_lp, events, event_id=classes_map, tmin=1, tmax=6, preload=True, baseline=None, reject=dict(eeg=100e-6)) #, baseline=(1,2))\n",
    "\n",
    "print(epochs)\n",
    "\n",
    "if plot:\n",
    "    epochs.plot(n_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resample to 10 Hz:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs_resampled = epochs.copy().resample(10)\n",
    "print('Preprocessing finished.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementing cue-aligned (better according to Reinmar paper)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Distance decoding:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "events = mne.find_events(raw_lp, stim_channel='Markers')\n",
    "event_dict = {'short': 1, 'long': 2, 'short': 1, 'long': 2, 'short':1, 'long':2, 'short':1, 'long':2}\n",
    "\n",
    "epochs_long_short = mne.Epochs(raw_lp, events, event_id=event_dict, tmin=1, tmax=6, preload=True, baseline=None, reject=dict(eeg=100e-6))\n",
    "\n",
    "\n",
    "\n",
    "short = epochs_long_short['short'].average()\n",
    "\n",
    "long = epochs_long_short['long'].average()\n",
    "\n",
    "#evokeds = dict(short=short, long=long)\n",
    "#mne.viz.plot_compare_evokeds(evokeds, picks='POz')\n",
    "\n",
    "evokeds2 = dict(short=list(epochs_long_short['short'].iter_evoked()),\n",
    "                long=list(epochs_long_short['long'].iter_evoked()))\n",
    "mne.viz.plot_compare_evokeds(evokeds2, combine='mean', picks=['Cz', 'C1', 'C2', 'FCz', 'CPz'], show_sensors='upper right')\n",
    "plt.savefig('distance_grand_averages.pdf')\n",
    "\n",
    "#['Pz', 'POz', 'PO3', 'PO4', 'P2', 'P1', 'P2', 'Oz', 'O1', 'O2']\n",
    "\n",
    "epochs_long_short = epochs_long_short.copy().resample(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i,epoch in enumerate(epochs_long_short):\n",
    "    #print(epoch.shape)\n",
    "    # Deleting EOG channels:\n",
    "    epoch = np.delete(epoch, 40, 0)\n",
    "    epoch = np.delete(epoch, 21, 0)\n",
    "    epoch = np.delete(epoch, 16, 0)\n",
    "    X.append(epoch[:61,:])\n",
    "    y.append(list(epochs_long_short[i].event_id.values())[0])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(y)\n",
    "\n",
    "for i,label in enumerate(y):\n",
    "    if label % 2 == 0:\n",
    "        y[i] = 0\n",
    "    else:\n",
    "        y[i] = 1\n",
    "\n",
    "print(y)\n",
    "\n",
    "\n",
    "# Split training and test set:\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "acc = []\n",
    "cv_scores = []\n",
    "for idx in range(len(X[0,0])):\n",
    "    x = X[:,:,idx]\n",
    "    # Reshape X to 2d array:\n",
    "    #nsamples, nx, ny = x.shape\n",
    "    #x = x.reshape((nsamples,nx*ny))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "    scores = cross_val_score(clf, x, y, cv=100)\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "\n",
    "print('Done')\n",
    "\n",
    "t = np.arange(len(acc))\n",
    "t = t/10\n",
    "#plt.plot(t, acc)\n",
    "\n",
    "plt.plot(t, cv_scores)\n",
    "\n",
    "window = 7\n",
    "\n",
    "ma = np.convolve(cv_scores, np.ones(window), 'valid') / window\n",
    "\n",
    "plt.plot(t[:-window+1], ma)\n",
    "plt.plot([2,2], [min(cv_scores), max(cv_scores)])\n",
    "plt.title('Single sample approach, 180-fold CV')\n",
    "plt.savefig('distance_acc_single.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5 point LDA\n",
    "X = []\n",
    "y = []\n",
    "for i,epoch in enumerate(epochs_long_short):\n",
    "    #print(epoch.shape)\n",
    "    # Deleting Marker channel:\n",
    "    # Deleting EOG channels:\n",
    "    epoch = np.delete(epoch, 40, 0)\n",
    "    epoch = np.delete(epoch, 21, 0)\n",
    "    epoch = np.delete(epoch, 16, 0)\n",
    "    X.append(epoch[:61,:])\n",
    "    y.append(list(epochs_long_short[i].event_id.values())[0])\n",
    "\n",
    "for i,label in enumerate(y):\n",
    "    if label % 2 == 0:\n",
    "        y[i] = 0\n",
    "    else:\n",
    "        y[i] = 1\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "# Split training and test set:\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "acc = []\n",
    "cv_scores = []\n",
    "for idx in range(len(X[0,0])-5):\n",
    "    x = X[:,:,idx:idx+5]\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "        print(x.shape)\n",
    "    # Reshape X to 2d array:\n",
    "    nsamples, nx, ny = x.shape\n",
    "    x = x.reshape((nsamples,nx*ny))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "    scores = cross_val_score(clf, x, y, cv=100)\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "\n",
    "\n",
    "print('Done')\n",
    "#print(acc)\n",
    "\n",
    "t = np.arange(len(acc))\n",
    "t = t/10 + 5/10\n",
    "#plt.plot(t, acc)\n",
    "\n",
    "plt.plot(t, cv_scores)\n",
    "\n",
    "window = 7\n",
    "\n",
    "ma = np.convolve(cv_scores, np.ones(window), 'valid') / window\n",
    "\n",
    "plt.plot(t[window-1:], ma)\n",
    "plt.plot([2,2], [min(cv_scores), max(cv_scores)])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Accuracy (a.u.)')\n",
    "plt.title('Windowed approach accuracies, distance 180-fold CV')\n",
    "plt.savefig('distance_acc_5point.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}