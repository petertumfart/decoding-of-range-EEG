{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import exists\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyxdf\n",
    "import mne\n",
    "from utils import *\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "print('Imports done...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Helper functions:\n",
    "def extract_eeg(stream, kick_last_ch=True):\n",
    "    \"\"\"\n",
    "    Extracts the EEG data and the EEG timestamp data from the stream and stores it into two lists.\n",
    "    :param stream: XDF stream containing the EEG data.\n",
    "    :param kick_last_ch: Boolean to kick out the brainproducts marker channel\n",
    "    :return: eeg: list containing the eeg data\n",
    "             eeg_ts: list containing the eeg timestamps.cd\n",
    "    \"\"\"\n",
    "    eeg = eeg_stream['time_series'].T\n",
    "    eeg *= 1e-6 # Convert to volts.\n",
    "    assert eeg.shape[0] == 65\n",
    "    eeg_ts = eeg_stream['time_stamps']\n",
    "\n",
    "    if kick_last_ch:\n",
    "        # Kick the last row (unused Brainproduct markers):\n",
    "        eeg = eeg[:64,:]\n",
    "\n",
    "    return eeg, eeg_ts\n",
    "\n",
    "\n",
    "def extract_eeg_infos(stream):\n",
    "    \"\"\"\n",
    "    Takes eeg stream and extracts the sampling rate, channel names, channel labels and the effective sample rate from the xdf info.\n",
    "    :param stream: EEG xdf stream\n",
    "    :return: sampling_rate: Configured sampling rate\n",
    "    :return: names: channel names\n",
    "    :return: labels: channel labels (eeg or eog)\n",
    "    :return: effective_sample_frequency: Actual sampling frequency based on timestamps.\n",
    "    \"\"\"\n",
    "    # Extract all infos from the EEG stream:\n",
    "    recording_device = stream['info']['name'][0]\n",
    "    sampling_rate = float(stream['info']['nominal_srate'][0])\n",
    "    effective_sample_frequency = float(stream['info']['effective_srate'])\n",
    "\n",
    "    # Extract channel names:\n",
    "    names = [stream['info']['desc'][0]['channels'][0]['channel'][i]['label'][0] for i in range(64)]\n",
    "    # chn_names.append('Markers')\n",
    "    labels = ['eeg' for i in range(64)]\n",
    "    labels[16] = 'eog'\n",
    "    labels[21] = 'eog'\n",
    "    labels[40] = 'eog'\n",
    "    # chn_labels.append('misc')\n",
    "\n",
    "    return sampling_rate, names, labels, effective_sample_frequency\n",
    "\n",
    "\n",
    "def extract_annotations(marker_stream, first_samp):\n",
    "    \"\"\"\n",
    "    Function to extract the triggers of the marker stream in order to prepare for the annotations.\n",
    "    :param marker_stream: xdf stream containing the markers and time_stamps\n",
    "    :param first_samp: First EEG sample, serves for aligning the markers\n",
    "    :return: triggers: Dict containing the extracted triggers.\n",
    "    \"\"\"\n",
    "    triggers = {'onsets': [], 'duration': [], 'description': []}\n",
    "\n",
    "    # Extract the markers:\n",
    "    markers = marker_stream['time_series']\n",
    "\n",
    "    # Extract the timestamp of the markers an correct them to zero\n",
    "    markers_ts = marker_stream['time_stamps'] - first_samp\n",
    "\n",
    "    # Read every trigger in the stream\n",
    "    for idx, marker_data in enumerate(markers):\n",
    "        # extract triggers information\n",
    "        triggers['onsets'].append(markers_ts[idx])\n",
    "        triggers['duration'].append(int(0))\n",
    "        # print(marker_data[0])\n",
    "        triggers['description'].append(marker_data[0])\n",
    "\n",
    "    return triggers\n",
    "\n",
    "def add_bad_channel_to_df(bad_chn_row, ch_names, csv_name='bad_channels.csv'):\n",
    "    \"\"\"\n",
    "    Add a row to a CSV file containing information about bad channels in some data.\n",
    "\n",
    "    :param bad_chn_row : list\n",
    "        A list containing the information to be added to the CSV file. The order of the elements should\n",
    "        match the order of the columns in the CSV file.\n",
    "    :param csv_name : str, optional\n",
    "        The name of the CSV file. The default is 'bad_channels.csv'.\n",
    "    :return: df_bads : pandas.DataFrame\n",
    "        A dataframe containing the information from the CSV file, with the new row added.\n",
    "    \"\"\"\n",
    "    # Check if df_bads.csv already exists:\n",
    "    if not exists(csv_name):\n",
    "        # Create dataframe with bad channels:\n",
    "        df_bads = pd.DataFrame(columns=['Subject', 'Run', 'Paradigm', 'Bad_channel'])\n",
    "        df_bads.to_csv(csv_name)\n",
    "    else:\n",
    "        # Load dataframe\n",
    "        df_bads = pd.read_csv(csv_name, index_col=0)\n",
    "\n",
    "    # Check if the channel name exists:\n",
    "    if bad_chn_row[-1] not in ch_names:\n",
    "        raise NameError('Channel name not found')\n",
    "\n",
    "    # Add row to the dataframe:\n",
    "    df_bads.loc[len(df_bads.index)] = bad_chn_row\n",
    "\n",
    "    print(f'Added {bad_chn_row} to the dataframe...')\n",
    "\n",
    "    # Drop duplicates:\n",
    "    df_bads.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Save df:\n",
    "    df_bads.to_csv(csv_name)\n",
    "\n",
    "    return df_bads\n",
    "\n",
    "def get_bads_for_subject(subject, csv_file='bad_channels.csv'):\n",
    "    \"\"\"\n",
    "    Get a list of bad channels that appear more than once for a given subject from a CSV file.\n",
    "\n",
    "    :param subject (str): Subject name.\n",
    "    :param csv_file (str): CSV file containing bad channel information. Default is 'bad_channels.csv'.\n",
    "\n",
    "    :returns: list: List of bad channels that appear more than once.\n",
    "\n",
    "    :raises: FileExistsError: If the CSV file does not exist.\n",
    "    \"\"\"\n",
    "    # Check if df_bads.csv already exists:\n",
    "    if not exists(csv_file):\n",
    "        raise FileExistsError('File does not exist, please use the add_bad_channel_df() function.')\n",
    "    else:\n",
    "        # Load dataframe\n",
    "        df = pd.read_csv(csv_file, index_col=0)\n",
    "\n",
    "    # Filter for subject and check if channel has more then 1 appearances:\n",
    "    subject_df = df[df['Subject'] == subject]\n",
    "\n",
    "    # Get the counts of all the unique values in the 'column_name' column\n",
    "    channel_counts = subject_df['Bad_channel'].value_counts()\n",
    "\n",
    "    # Select the rows that have a count greater than 1\n",
    "    duplicate_bads = list(channel_counts[channel_counts>1].index)\n",
    "\n",
    "    return duplicate_bads\n",
    "\n",
    "def get_all_additional_information(subject, csv_file='participant_info.csv'):\n",
    "    # Open participant info:\n",
    "    if not exists(csv_file):\n",
    "        raise FileExistsError('File does not exist. Check if the path is correct.')\n",
    "    else:\n",
    "        # Load dataframe\n",
    "        df = pd.read_csv(csv_file, index_col=False)\n",
    "\n",
    "    meas_date_str = df[df['Participant'] == subject]['Measurement_Date'].values[0]\n",
    "    meas_date = datetime.strptime(meas_date_str, '%d.%m.%Y')\n",
    "    meas_date = meas_date.replace(tzinfo=timezone.utc)\n",
    "    experimenter = 'Peter T.'\n",
    "    proj_name = 'Decoding of range during goal-directed movement'\n",
    "    subject_info = subject\n",
    "    line_freq = 50.0\n",
    "    gender = df[df['Participant'] == subject]['Gender'].values[0]\n",
    "    dob = df[df['Participant'] == subject]['Date_Of_Birth'].values[0]\n",
    "    age_at_meas = df[df['Participant'] == subject]['Age_At_Measurement'].values[0]\n",
    "\n",
    "    return meas_date, experimenter, proj_name, subject_info, line_freq, gender, dob, age_at_meas\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Constants"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data_path = 'C:/Users/tumfart/Code/github/master-thesis/data/'\n",
    "data_path = 'C:/Users/peter/Google Drive/measurements/eeg/'\n",
    "subjects = ['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07' , 'A08', 'A09', 'A10']\n",
    "# = 'A03'\n",
    "paradigm = 'paradigm' # 'eye', 'paradigm'\n",
    "plot = False\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "# Create path list for each subject:\n",
    "paths = [str(data_path + subject + '/' + paradigm) for subject in subjects]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read xdf-files for specified subject"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Extracting subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '.xdf' in f]\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "        file = path + '/' + file_name\n",
    "\n",
    "        # Read the raw stream:\n",
    "        streams, header = pyxdf.load_xdf(file)\n",
    "\n",
    "        # Split the streams:\n",
    "        eeg_stream, marker_stream = split_streams(streams)\n",
    "\n",
    "        # Get the eeg data:\n",
    "        eeg, eeg_ts = extract_eeg(eeg_stream, kick_last_ch=True)\n",
    "        #max_eeg_ts.append(eeg_ts.max())\n",
    "\n",
    "        # Extract all infos from the EEG stream:\n",
    "        fs, ch_names, ch_labels, eff_fs = extract_eeg_infos(eeg_stream)\n",
    "\n",
    "        # Extract the triggers from the marker stream:\n",
    "        triggers = extract_annotations(marker_stream, first_samp=eeg_ts[0])\n",
    "\n",
    "        # Define MNE annotations\n",
    "        annotations = mne.Annotations(triggers['onsets'], triggers['duration'], triggers['description'], orig_time=None)\n",
    "\n",
    "        # Create mne info:\n",
    "        # TODO: Check what info can be added to the stream:\n",
    "        info = mne.create_info(ch_names, fs, ch_labels)\n",
    "\n",
    "        # Create the raw array and add info, montage and annotations:\n",
    "        raw = mne.io.RawArray(eeg, info, first_samp=eeg_ts[0])\n",
    "        raw.set_montage('standard_1005')\n",
    "        raw.set_annotations(annotations)\n",
    "\n",
    "        # Store the raw file:\n",
    "        store_name = path + '/' + subject + '_run_' + str(i + 1) + '_unprocessed_raw.fif'\n",
    "        raw.save(store_name, overwrite=True)\n",
    "\n",
    "        if plot:\n",
    "            raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                     remove_dc=False, title='Raw')\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished reading, took me {round(time.time()-start)} seconds...')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter the signals"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and load the raw files:\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading raw files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_unprocessed_raw.fif' in f]\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "        if plot:\n",
    "            raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                              remove_dc=False, title='Highpass filtered')\n",
    "            plot_spectrum(raw)\n",
    "\n",
    "\n",
    "        # Highpass filter:\n",
    "        raw_highpass = raw.copy().filter(l_freq=0.4, h_freq=None, picks=['eeg'], method='iir')\n",
    "        if plot:\n",
    "            raw_highpass.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                              remove_dc=False, title='Highpass filtered')\n",
    "            plot_spectrum(raw_highpass)\n",
    "\n",
    "        # Notch filter:\n",
    "        raw_notch = raw_highpass.copy().notch_filter(freqs=[50], picks=['eeg'])\n",
    "        if plot:\n",
    "            raw_notch.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title='Notch filtered')\n",
    "            plot_spectrum(raw_notch)\n",
    "\n",
    "        # Store the raw file:\n",
    "        store_name = path + '/' + subject + '_run_' + str(i + 1) + '_highpass_notch_filtered_raw.fif'\n",
    "        raw_notch.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished highpass and notch filtering, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize signals for bad channel identification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specifiy subject:\n",
    "subject = 'A01'\n",
    "paradigm = 'paradigm'\n",
    "if paradigm == 'paradigm':\n",
    "    runs = 9\n",
    "else:\n",
    "    runs = 2\n",
    "names = [subject + '_run_' + str(i + 1) + '_highpass_notch_filtered_raw.fif' for i in range(runs)]\n",
    "\n",
    "for i, name in enumerate(names):\n",
    "    file = data_path + subject + '/' + paradigm + '/' + name\n",
    "    raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "    raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title=f'Notch & HP filtered. Run: {i+1}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specifiy subject:\n",
    "subject = 'A10'\n",
    "paradigm = 'eye'\n",
    "run = 2\n",
    "name = subject + '_run_' + str(run) + '_highpass_notch_filtered_raw.fif'\n",
    "file = data_path + subject + '/' + paradigm + '/' + name\n",
    "\n",
    "raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title=f'Notch & HP filtered. Run: {run}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add bad channel to bad channel.csv:\n",
    "bad_df = add_bad_channel_to_df([subject, run, paradigm, 'T8'], ch_names=raw.ch_names, csv_name='bad_channels.csv')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add bad channels to all raw infos:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '.fif' in f]\n",
    "    # Add bad channels:\n",
    "    bads = get_bads_for_subject(subject, csv_file='bad_channels.csv')\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "        raw.info['bads'] = bads\n",
    "\n",
    "        # Overwrite the raw file with the added info:\n",
    "        store_name = path + '/' + file_name\n",
    "        raw.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished bad channel adding, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Perform interpolation of bad channels:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_highpass_notch_filtered_raw.fif' in f]\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "        # Interpolate bad channels:\n",
    "        raw_interp = raw.copy().interpolate_bads(reset_bads=False)\n",
    "\n",
    "        # Overwrite the raw file with the added info:\n",
    "        store_name = path + '/' + subject + '_run_' + str(i + 1) + '_bad_channels_interpolated_raw.fif'\n",
    "        raw_interp.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished interpolating bad channels, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CAR re-referencing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "\n",
    "    #TODO: Loaded fif file changes in final pipeline (because eye artifact correction was not yet implemented).\n",
    "    file_names = [f for f in listdir(path) if '_bad_channels_interpolated_raw.fif' in f]\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw_interp = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "        # Interpolate bad channels:\n",
    "        raw_avg_ref = raw_interp.copy().set_eeg_reference(ref_channels='average')\n",
    "\n",
    "        # Overwrite the raw file with the added info:\n",
    "        store_name = path + '/' + subject + '_run_' + str(i + 1) + '_car_referenced_raw.fif'\n",
    "        raw_avg_ref.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished rereferencing eeg, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load uninterpolated raw and interpolated raw:\n",
    "raw_avg_ref = mne.io.read_raw('C:/Users/peter/Google Drive/measurements/eeg/A01/paradigm/A01_run_1_car_referenced_raw.fif')\n",
    "raw_interp = mne.io.read_raw('C:/Users/peter/Google Drive/measurements/eeg/A01/paradigm/A01_run_1_bad_channels_interpolated_raw.fif')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_avg_ref.plot(duration=60, proj=False, n_channels=len(raw_avg_ref.ch_names), remove_dc=False, title=f'CAR referenced.')\n",
    "raw_interp.plot(duration=60, proj=False, n_channels=len(raw_interp.ch_names), remove_dc=False, title='Interpolated')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper cell to add info:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '.fif' in f]\n",
    "\n",
    "    # Get correct info:\n",
    "    meas_date, experimenter, proj_name, subject_info, line_freq, gender, dob, age_at_meas = get_all_additional_information(subject, csv_file='participant_info.csv')\n",
    "\n",
    "    big_subject_info = {'Subject ID': subject_info,\n",
    "                        'Gender': gender,\n",
    "                        'Age at measurement': age_at_meas}\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "        # Add infos:\n",
    "        raw.info['subject_info'] = big_subject_info\n",
    "        raw.info['experimenter'] = experimenter\n",
    "        #raw.info['proj_name'] = proj_name\n",
    "        raw.set_meas_date(meas_date)\n",
    "        raw.info['line_freq'] = line_freq\n",
    "\n",
    "        # Overwrite the raw file with the added info:\n",
    "        store_name = path + '/' + file_name\n",
    "        raw.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished adding info, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List files in folder:\n",
    "files = [f for f in listdir(path)]\n",
    "\n",
    "eeg_streams = []\n",
    "marker_streams = []\n",
    "# Load all recorded EEG files for one subjectc\n",
    "files = [files[0]]\n",
    "for file in files:\n",
    "    file_name = path + '/' + file\n",
    "    print(f'####', end='#')\n",
    "\n",
    "    # Read streams\n",
    "    streams, header = pyxdf.load_xdf(file_name)\n",
    "\n",
    "    # Split the streams:\n",
    "    eeg_stream, marker_stream = split_streams(streams)\n",
    "\n",
    "    eeg_streams.append(eeg_stream)\n",
    "    marker_streams.append(marker_stream)\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Finished reading, found {len(eeg_streams)} EEG streams and {len(marker_streams)} marker streams...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "differences = [0]\n",
    "max_eeg_ts = []\n",
    "for i, (eeg_stream, m_stream) in enumerate(zip(eeg_streams, marker_streams)):\n",
    "    # Get the eeg data:\n",
    "    eeg, eeg_ts = extract_eeg(eeg_stream)\n",
    "    max_eeg_ts.append(eeg_ts.max())\n",
    "\n",
    "    # Kick the last row (unused Brainproduct markers):\n",
    "    eeg = eeg[:64,:]\n",
    "\n",
    "    # Extract all infos from the EEG stream:\n",
    "    fs, ch_names, ch_labels, eff_fs = extract_eeg_infos(eeg_stream)\n",
    "\n",
    "    # Extract the markers and timestamps:\n",
    "    # markers = m_stream['time_series']\n",
    "    # markers_ts = m_stream['time_stamps']\n",
    "    #\n",
    "    # # Convert list of list of strings to list of strings:\n",
    "    # markers = [''.join(element) for element in markers]\n",
    "\n",
    "    # # Make Nan array with len(eeg)\n",
    "    # aligned_markers = np.empty(eeg_ts.shape, dtype='<U5')\n",
    "    #\n",
    "    # # Place markers string at the align array where first time markers_ts <= eeg_ts:\n",
    "    # for k, marker in enumerate(markers):\n",
    "    #     ts = markers_ts[k]\n",
    "    #     idx = np.where(ts <= eeg_ts)[0][0]\n",
    "    #     aligned_markers[idx] = marker\n",
    "\n",
    "    if i == 0:\n",
    "        global_eeg = eeg\n",
    "        first_ts = eeg_ts[0]\n",
    "        # global_markers = aligned_markers\n",
    "    else:\n",
    "        global_eeg = np.concatenate((global_eeg, eeg), axis=1)\n",
    "        # global_markers = np.concatenate((global_markers, aligned_markers))\n",
    "        differences.append(eeg_ts[0]-last_ts)\n",
    "\n",
    "    last_ts = eeg_ts[-1]\n",
    "    print(f'####', end='#')\n",
    "\n",
    "cum_diff = np.cumsum(differences)\n",
    "eeg = global_eeg\n",
    "# markers = global_markers\n",
    "print()\n",
    "print('Extracted EEG data, EEG infos...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# annotation generation from:\n",
    "# https://github.com/WriessneggerLab/EEG-preprocessing/blob/eeg/src/EEGAnalysis.py\n",
    "# generation of the events according to the definition\n",
    "triggers = {'onsets': [], 'duration': [], 'description': []}\n",
    "global_markers_ts = []\n",
    "for i, m_stream in enumerate(marker_streams):\n",
    "    # Extract the markers and timestamps:\n",
    "    markers = m_stream['time_series']\n",
    "    markers_ts = m_stream['time_stamps'] - float(m_stream['info']['created_at'][0])# - cum_diff[i]\n",
    "\n",
    "\n",
    "    global_markers_ts += list(markers_ts)\n",
    "    # read every trigger in the stream\n",
    "    for idx, marker_data in enumerate(markers):\n",
    "        # extract triggers information\n",
    "        triggers['onsets'].append(markers_ts[idx])\n",
    "        triggers['duration'].append(int(0))\n",
    "        # print(marker_data[0])\n",
    "        triggers['description'].append(marker_data[0])\n",
    "\n",
    "# define MNE annotations\n",
    "annotations = mne.Annotations(triggers['onsets'], triggers['duration'], triggers['description'], orig_time=None) #, orig_time=np.array(global_markers_ts))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mrks_list = list(markers_ts)\n",
    "a = []\n",
    "a += mrks_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Put extracted data into mne structure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: align annotations\n",
    "\n",
    "info = mne.create_info(ch_names, fs, ch_labels)\n",
    "\n",
    "raw = mne.io.RawArray(eeg, info, first_samp=first_ts)\n",
    "raw.set_montage('standard_1005')\n",
    "raw.set_annotations(annotations)\n",
    "\n",
    "if plot:\n",
    "    raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "             remove_dc=False, title='Raw')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter with HP at 0.4Hz and BS at 50 Hz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_highpass = raw.copy().filter(l_freq=0.4, h_freq=None, picks=['eeg'], method='iir')\n",
    "if plot:\n",
    "    raw_highpass.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                      remove_dc=False, title='Highpass filtered')\n",
    "    plot_spectrum(raw_highpass)\n",
    "\n",
    "raw_notch = raw_highpass.copy().notch_filter(freqs=[50], picks=['eeg'])\n",
    "if plot:\n",
    "    raw_notch.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title='Notch filtered')\n",
    "    plot_spectrum(raw_notch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Interpolate bad channels:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: check function --> need to mark them first\n",
    "raw_interp = raw_notch.copy().interpolate_bads(reset_bads=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correct eye artifacts:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CAR:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_avg_ref = raw_interp.copy().set_eeg_reference(ref_channels='average')\n",
    "if plot:\n",
    "    raw_avg_ref.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title='CAR Referenced')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HEAR model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LP at 3.0Hz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_lp = raw_avg_ref.copy().filter(l_freq=None, h_freq=3.0, picks=['eeg'], method='iir')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract epochs before resampling (otherwise markers may get lost) and reject bad trials:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "events = mne.find_events(raw_lp, stim_channel='Markers')\n",
    "\n",
    "epochs = mne.Epochs(raw_lp, events, event_id=classes_map, tmin=1, tmax=6, preload=True, baseline=None, reject=dict(eeg=100e-6)) #, baseline=(1,2))\n",
    "\n",
    "print(epochs)\n",
    "\n",
    "if plot:\n",
    "    epochs.plot(n_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resample to 10 Hz:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs_resampled = epochs.copy().resample(10)\n",
    "print('Preprocessing finished.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementing cue-aligned (better according to Reinmar paper)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Distance decoding:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "events = mne.find_events(raw_lp, stim_channel='Markers')\n",
    "event_dict = {'short': 1, 'long': 2, 'short': 1, 'long': 2, 'short':1, 'long':2, 'short':1, 'long':2}\n",
    "\n",
    "epochs_long_short = mne.Epochs(raw_lp, events, event_id=event_dict, tmin=1, tmax=6, preload=True, baseline=None, reject=dict(eeg=100e-6))\n",
    "\n",
    "\n",
    "\n",
    "short = epochs_long_short['short'].average()\n",
    "\n",
    "long = epochs_long_short['long'].average()\n",
    "\n",
    "#evokeds = dict(short=short, long=long)\n",
    "#mne.viz.plot_compare_evokeds(evokeds, picks='POz')\n",
    "\n",
    "evokeds2 = dict(short=list(epochs_long_short['short'].iter_evoked()),\n",
    "                long=list(epochs_long_short['long'].iter_evoked()))\n",
    "mne.viz.plot_compare_evokeds(evokeds2, combine='mean', picks=['Cz', 'C1', 'C2', 'FCz', 'CPz'], show_sensors='upper right')\n",
    "plt.savefig('distance_grand_averages.pdf')\n",
    "\n",
    "#['Pz', 'POz', 'PO3', 'PO4', 'P2', 'P1', 'P2', 'Oz', 'O1', 'O2']\n",
    "\n",
    "epochs_long_short = epochs_long_short.copy().resample(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i,epoch in enumerate(epochs_long_short):\n",
    "    #print(epoch.shape)\n",
    "    # Deleting EOG channels:\n",
    "    epoch = np.delete(epoch, 40, 0)\n",
    "    epoch = np.delete(epoch, 21, 0)\n",
    "    epoch = np.delete(epoch, 16, 0)\n",
    "    X.append(epoch[:61,:])\n",
    "    y.append(list(epochs_long_short[i].event_id.values())[0])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(y)\n",
    "\n",
    "for i,label in enumerate(y):\n",
    "    if label % 2 == 0:\n",
    "        y[i] = 0\n",
    "    else:\n",
    "        y[i] = 1\n",
    "\n",
    "print(y)\n",
    "\n",
    "\n",
    "# Split training and test set:\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "acc = []\n",
    "cv_scores = []\n",
    "for idx in range(len(X[0,0])):\n",
    "    x = X[:,:,idx]\n",
    "    # Reshape X to 2d array:\n",
    "    #nsamples, nx, ny = x.shape\n",
    "    #x = x.reshape((nsamples,nx*ny))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "    scores = cross_val_score(clf, x, y, cv=100)\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "\n",
    "print('Done')\n",
    "\n",
    "t = np.arange(len(acc))\n",
    "t = t/10\n",
    "#plt.plot(t, acc)\n",
    "\n",
    "plt.plot(t, cv_scores)\n",
    "\n",
    "window = 7\n",
    "\n",
    "ma = np.convolve(cv_scores, np.ones(window), 'valid') / window\n",
    "\n",
    "plt.plot(t[:-window+1], ma)\n",
    "plt.plot([2,2], [min(cv_scores), max(cv_scores)])\n",
    "plt.title('Single sample approach, 180-fold CV')\n",
    "plt.savefig('distance_acc_single.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5 point LDA\n",
    "X = []\n",
    "y = []\n",
    "for i,epoch in enumerate(epochs_long_short):\n",
    "    #print(epoch.shape)\n",
    "    # Deleting Marker channel:\n",
    "    # Deleting EOG channels:\n",
    "    epoch = np.delete(epoch, 40, 0)\n",
    "    epoch = np.delete(epoch, 21, 0)\n",
    "    epoch = np.delete(epoch, 16, 0)\n",
    "    X.append(epoch[:61,:])\n",
    "    y.append(list(epochs_long_short[i].event_id.values())[0])\n",
    "\n",
    "for i,label in enumerate(y):\n",
    "    if label % 2 == 0:\n",
    "        y[i] = 0\n",
    "    else:\n",
    "        y[i] = 1\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "# Split training and test set:\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "acc = []\n",
    "cv_scores = []\n",
    "for idx in range(len(X[0,0])-5):\n",
    "    x = X[:,:,idx:idx+5]\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "        print(x.shape)\n",
    "    # Reshape X to 2d array:\n",
    "    nsamples, nx, ny = x.shape\n",
    "    x = x.reshape((nsamples,nx*ny))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "    scores = cross_val_score(clf, x, y, cv=100)\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "\n",
    "\n",
    "print('Done')\n",
    "#print(acc)\n",
    "\n",
    "t = np.arange(len(acc))\n",
    "t = t/10 + 5/10\n",
    "#plt.plot(t, acc)\n",
    "\n",
    "plt.plot(t, cv_scores)\n",
    "\n",
    "window = 7\n",
    "\n",
    "ma = np.convolve(cv_scores, np.ones(window), 'valid') / window\n",
    "\n",
    "plt.plot(t[window-1:], ma)\n",
    "plt.plot([2,2], [min(cv_scores), max(cv_scores)])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Accuracy (a.u.)')\n",
    "plt.title('Windowed approach accuracies, distance 180-fold CV')\n",
    "plt.savefig('distance_acc_5point.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}