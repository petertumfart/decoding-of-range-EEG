{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import exists\n",
    "from importlib import reload\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyxdf\n",
    "import mne\n",
    "from utils import *\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime, timezone\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "\n",
    "print('Imports done...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Helper functions:\n",
    "def extract_eeg(stream, kick_last_ch=True):\n",
    "    \"\"\"\n",
    "    Extracts the EEG data and the EEG timestamp data from the stream and stores it into two lists.\n",
    "    :param stream: XDF stream containing the EEG data.\n",
    "    :param kick_last_ch: Boolean to kick out the brainproducts marker channel\n",
    "    :return: eeg: list containing the eeg data\n",
    "             eeg_ts: list containing the eeg timestamps.cd\n",
    "    \"\"\"\n",
    "    extr_eeg = stream['time_series'].T\n",
    "    extr_eeg *= 1e-6 # Convert to volts.\n",
    "    assert extr_eeg.shape[0] == 65\n",
    "    extr_eeg_ts = eeg_stream['time_stamps']\n",
    "\n",
    "    if kick_last_ch:\n",
    "        # Kick the last row (unused Brainproduct markers):\n",
    "        extr_eeg = extr_eeg[:64,:]\n",
    "\n",
    "    return extr_eeg, extr_eeg_ts\n",
    "\n",
    "\n",
    "def extract_eeg_infos(stream):\n",
    "    \"\"\"\n",
    "    Takes eeg stream and extracts the sampling rate, channel names, channel labels and the effective sample rate from the xdf info.\n",
    "    :param stream: EEG xdf stream\n",
    "    :return: sampling_rate: Configured sampling rate\n",
    "    :return: names: channel names\n",
    "    :return: labels: channel labels (eeg or eog)\n",
    "    :return: effective_sample_frequency: Actual sampling frequency based on timestamps.\n",
    "    \"\"\"\n",
    "    # Extract all infos from the EEG stream:\n",
    "    recording_device = stream['info']['name'][0]\n",
    "    sampling_rate = float(stream['info']['nominal_srate'][0])\n",
    "    effective_sample_frequency = float(stream['info']['effective_srate'])\n",
    "\n",
    "    # Extract channel names:\n",
    "    chn_names = [stream['info']['desc'][0]['channels'][0]['channel'][i]['label'][0] for i in range(64)]\n",
    "    # chn_names.append('Markers')\n",
    "    labels = ['eeg' for i in range(64)]\n",
    "    labels[16] = 'eog'\n",
    "    labels[21] = 'eog'\n",
    "    labels[40] = 'eog'\n",
    "    # chn_labels.append('misc')\n",
    "\n",
    "    return sampling_rate, chn_names, labels, effective_sample_frequency\n",
    "\n",
    "\n",
    "def extract_annotations(mark_stream, first_samp):\n",
    "    \"\"\"\n",
    "    Function to extract the triggers of the marker stream in order to prepare for the annotations.\n",
    "    :param mark_stream: xdf stream containing the markers and time_stamps\n",
    "    :param first_samp: First EEG sample, serves for aligning the markers\n",
    "    :return: triggs: Dict containing the extracted triggers.\n",
    "    \"\"\"\n",
    "    triggs = {'onsets': [], 'duration': [], 'description': []}\n",
    "\n",
    "    # Extract the markers:\n",
    "    marks = mark_stream['time_series']\n",
    "\n",
    "    # Fix markers due to bug in paradigm:\n",
    "    corrected_markers = fix_markers(marks)\n",
    "\n",
    "    # Extract the timestamp of the markers and correct them to zero\n",
    "    marks_ts = mark_stream['time_stamps'] - first_samp\n",
    "\n",
    "    # Read every trigger in the stream\n",
    "    for index, marker_data in enumerate(corrected_markers):\n",
    "        # extract triggers information\n",
    "        triggs['onsets'].append(marks_ts[index])\n",
    "        triggs['duration'].append(int(0))\n",
    "        # print(marker_data[0])\n",
    "        triggs['description'].append(marker_data[0])\n",
    "\n",
    "    return triggs\n",
    "\n",
    "# Fix markers:\n",
    "def fix_markers(orig_markers):\n",
    "    \"\"\"\n",
    "    Given a list of markers, this function processes the markers and modifies the trial type markers if necessary.\n",
    "    Due to shuffle-bug in the paradigm.\n",
    "\n",
    "    :param orig_markers: A list of markers. Each marker is a tuple containing the marker string and a float value representing the time at which the marker occurred.\n",
    "    :type orig_markers: list\n",
    "    :return: The modified list of markers.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "\n",
    "    trial_type_markers = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "    counter_letter = {'l': 'R', 'r': 'L', 'b': 'T', 't': 'B'}\n",
    "\n",
    "    # Parse through markers\n",
    "    for i in range(len(orig_markers)-3):\n",
    "        marker = orig_markers[i][0]\n",
    "        if marker in trial_type_markers:\n",
    "            following_markers = []\n",
    "            # Find the next 4 occurances that start with 'c':\n",
    "            # and store them in a list:\n",
    "            if (i+9) < len(orig_markers):\n",
    "                for ii in range(i+1, i+9):\n",
    "                    next_mark = orig_markers[ii][0]\n",
    "                    if next_mark[0] == 'c':\n",
    "                        following_markers.append(next_mark[2])\n",
    "            else:\n",
    "                for ii in range(i+1, len(orig_markers)):\n",
    "                    next_mark = orig_markers[ii][0]\n",
    "                    if next_mark[0] == 'c':\n",
    "                        following_markers.append(next_mark[2])\n",
    "\n",
    "            # Exit loop if less than 4 following markers were found:\n",
    "            if len(following_markers) < 4:\n",
    "                continue\n",
    "\n",
    "            if following_markers[0] == 'c' or following_markers[1] == 'c':\n",
    "                continue\n",
    "\n",
    "            # Extract first letter of the trial type marker:\n",
    "            first_letter = marker[0].lower()\n",
    "            last_letter = marker[-1].lower()\n",
    "\n",
    "            # Check if the first two letters in following markers are the same, if not, change type:\n",
    "            if (following_markers[0] != first_letter) and (following_markers[1] != first_letter):\n",
    "                # Trial type changes:\n",
    "                new_type = following_markers[0].upper() + 'T' + counter_letter[following_markers[0]] + '-'\n",
    "\n",
    "                if (following_markers[2] == 'c') and (following_markers[3] == 'c'):\n",
    "                    new_type = new_type + 's'\n",
    "                else:\n",
    "                    new_type = new_type + 'l'\n",
    "\n",
    "                orig_markers[i][0] = new_type\n",
    "\n",
    "            # Otherwise check if the second two markers are short or long and change accordingly:\n",
    "            else:\n",
    "                if (last_letter == 's') and (following_markers[2] != 'c') and (following_markers[3] != 'c'):\n",
    "                    new_type = marker[:-1]\n",
    "                    new_type += 'l'\n",
    "                    orig_markers[i][0] = new_type\n",
    "\n",
    "                elif (last_letter == 'l') and (following_markers[2] == 'c') and (following_markers[3] == 'c'):\n",
    "                    new_type = marker[:-1]\n",
    "                    new_type += 's'\n",
    "                    orig_markers[i][0] = new_type\n",
    "\n",
    "\n",
    "    return orig_markers\n",
    "\n",
    "def add_bad_channel_to_df(bad_chn_row, ch_names, csv_name='bad_channels.csv'):\n",
    "    \"\"\"\n",
    "    Add a row to a CSV file containing information about bad channels in some data.\n",
    "\n",
    "    :param bad_chn_row : list\n",
    "        A list containing the information to be added to the CSV file. The order of the elements should\n",
    "        match the order of the columns in the CSV file.\n",
    "    :param csv_name : str, optional\n",
    "        The name of the CSV file. The default is 'bad_channels.csv'.\n",
    "    :return: df_bads : pandas.DataFrame\n",
    "        A dataframe containing the information from the CSV file, with the new row added.\n",
    "    \"\"\"\n",
    "    # Check if df_bads.csv already exists:\n",
    "    if not exists(csv_name):\n",
    "        # Create dataframe with bad channels:\n",
    "        df_bads = pd.DataFrame(columns=['Subject', 'Run', 'Paradigm', 'Bad_channel'])\n",
    "        df_bads.to_csv(csv_name)\n",
    "    else:\n",
    "        # Load dataframe\n",
    "        df_bads = pd.read_csv(csv_name, index_col=0)\n",
    "\n",
    "    # Check if the channel name exists:\n",
    "    if bad_chn_row[-1] not in ch_names:\n",
    "        raise NameError('Channel name not found')\n",
    "\n",
    "    # Add row to the dataframe:\n",
    "    df_bads.loc[len(df_bads.index)] = bad_chn_row\n",
    "\n",
    "    print(f'Added {bad_chn_row} to the dataframe...')\n",
    "\n",
    "    # Drop duplicates:\n",
    "    df_bads.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Save df:\n",
    "    df_bads.to_csv(csv_name)\n",
    "\n",
    "    return df_bads\n",
    "\n",
    "def add_bad_epoch_to_df(bad_epoch_row, csv_name='bad_epochs.csv'):\n",
    "    # Check if bad_epochs.csv already exists:\n",
    "    if not exists(csv_name):\n",
    "        # Create dataframe with bad channels:\n",
    "        df_bads = pd.DataFrame(columns=['Subject', 'Paradigm', 'Bad epoch'])\n",
    "        df_bads.to_csv(csv_name)\n",
    "    else:\n",
    "        # Load dataframe\n",
    "        df_bads = pd.read_csv(csv_name, index_col=0)\n",
    "\n",
    "    # Add row to the dataframe:\n",
    "    df_bads.loc[len(df_bads.index)] = bad_epoch_row\n",
    "\n",
    "    print(f'Added {bad_epoch_row} to the dataframe...')\n",
    "\n",
    "    # Drop duplicates:\n",
    "    df_bads.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Save df:\n",
    "    df_bads.to_csv(csv_name)\n",
    "\n",
    "    return df_bads\n",
    "\n",
    "\n",
    "\n",
    "def get_bads_for_subject(subject, csv_file='bad_channels.csv'):\n",
    "    \"\"\"\n",
    "    Get a list of bad channels that appear more than once for a given subject from a CSV file.\n",
    "\n",
    "    :param subject: Subject name.\n",
    "    :type subject: str\n",
    "    :param csv_file: CSV file containing bad channel information. Default is 'bad_channels.csv'.\n",
    "    :type: csv_file: str\n",
    "\n",
    "    :returns: list: List of bad channels that appear more than once.\n",
    "\n",
    "    :raises: FileExistsError: If the CSV file does not exist.\n",
    "    \"\"\"\n",
    "    # Check if df_bads.csv already exists:\n",
    "    if not exists(csv_file):\n",
    "        raise FileExistsError('File does not exist, please use the add_bad_channel_df() function.')\n",
    "    else:\n",
    "        # Load dataframe\n",
    "        df = pd.read_csv(csv_file, index_col=0)\n",
    "\n",
    "    # Filter for subject and check if channel has more then 1 appearances:\n",
    "    subject_df = df[df['Subject'] == subject]\n",
    "\n",
    "    # Get the counts of all the unique values in the 'column_name' column\n",
    "    channel_counts = subject_df['Bad_channel'].value_counts()\n",
    "\n",
    "    # Select the rows that have a count greater than 1\n",
    "    duplicate_bads = list(channel_counts[channel_counts>1].index)\n",
    "\n",
    "    return duplicate_bads\n",
    "\n",
    "def get_all_additional_information(subject, csv_file='participant_info.csv'):\n",
    "    \"\"\"Returns a tuple of additional information for the given subject.\n",
    "\n",
    "    :param subject: The name of the subject.\n",
    "    :type subject: str\n",
    "    :param csv_file: The file path to the participant info CSV file.\n",
    "    :type csv_file: str\n",
    "    :return: A tuple containing the following information:\n",
    "        - meas_date (datetime): The measurement date.\n",
    "        - experimenter (str): The name of the experimenter.\n",
    "        - proj_name (str): The name of the project.\n",
    "        - subject_info (str): The name of the subject.\n",
    "        - line_freq (float): The line frequency.\n",
    "        - gender (str): The gender of the subject.\n",
    "        - dob (str): The date of birth of the subject.\n",
    "        - age_at_meas (float): The age of the subject at the time of measurement.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    if not isinstance(subject, str):\n",
    "        raise TypeError('Subject must be a string.')\n",
    "    if not isinstance(csv_file, str):\n",
    "        raise TypeError('CSV file must be a string.')\n",
    "    if not exists(csv_file):\n",
    "        raise FileNotFoundError('File does not exist. Check if the path is correct.')\n",
    "\n",
    "    df = pd.read_csv(csv_file, index_col=False)\n",
    "    subject_info = df[df['Participant'] == subject]\n",
    "\n",
    "    if subject_info.empty:\n",
    "        raise ValueError('Subject not found in CSV file.')\n",
    "\n",
    "    meas_date_str = subject_info['Measurement_Date'].values[0]\n",
    "    meas_date = datetime.strptime(meas_date_str, '%d.%m.%Y')\n",
    "    meas_date = meas_date.replace(tzinfo=timezone.utc)\n",
    "    experimenter = 'Peter T.'\n",
    "    proj_name = 'Decoding of range during goal-directed movement'\n",
    "    line_freq = 50.0\n",
    "    gender = subject_info['Gender'].values[0]\n",
    "    dob = subject_info['Date_Of_Birth'].values[0]\n",
    "    age_at_meas = subject_info['Age_At_Measurement'].values[0]\n",
    "\n",
    "    return meas_date, experimenter, proj_name, subject_info, line_freq, gender, dob, age_at_meas\n",
    "\n",
    "def get_subset_of_dict(full_dict, keys_of_interest):\n",
    "    return dict((k, full_dict[k]) for k in keys_of_interest if k in full_dict)\n",
    "\n",
    "\n",
    "def create_sliced_trial_list(event_dict, events_from_annot):\n",
    "    # Slice into list of list from trial_type_marker to trial_type_marker\n",
    "    trial_type_markers = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "    event_dict_trial_type = get_subset_of_dict(event_dict, trial_type_markers)\n",
    "    event_sequence = events_from_annot[:,-1]\n",
    "\n",
    "    trial_list = []\n",
    "    first_samps = []\n",
    "    first_time = True\n",
    "    for i, entry in enumerate(event_sequence):\n",
    "        if entry in event_dict_trial_type.values():\n",
    "            if first_time:\n",
    "                temp_list = [entry]\n",
    "                first_samps.append(events_from_annot[i,0])\n",
    "                first_time = False\n",
    "            else:\n",
    "                temp_list.append(entry)\n",
    "                trial_list.append(temp_list)\n",
    "                temp_list = [entry]\n",
    "                first_samps.append(events_from_annot[i,0])\n",
    "        else:\n",
    "            if not first_time:\n",
    "                temp_list.append(entry)\n",
    "\n",
    "    trial_list.append(temp_list)\n",
    "\n",
    "    return trial_list, first_samps\n",
    "\n",
    "\n",
    "def get_bad_epochs(event_dict, trial_list):\n",
    "    \"\"\"\n",
    "    Given an event dictionary, find the indices of the epochs (sub-lists) in the trial list that are invalid.\n",
    "    An epoch is invalid if it does not satisfy the following conditions:\n",
    "        1. If it is not the last epoch, its length must be 9.\n",
    "        2. If it is the last epoch, its length must be 8.\n",
    "        3. The first entry must be a trial_type marker.\n",
    "        4. The second entry must be the 'Start' marker.\n",
    "        5. The fourth entry must be the 'Cue' marker.\n",
    "        6. The seventh entry must be the 'Break' marker.\n",
    "        7. The first two LDR readings must be coherent with the trial type.\n",
    "        8. The second two LDR readings must be coherent with the trial type.\n",
    "\n",
    "    :param event_dict: A dictionary where keys are event names and values are corresponding event markers.\n",
    "    :type event_dict: dict\n",
    "    :return: A list of indices corresponding to the invalid epochs.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the order is correct:\n",
    "    bad_idcs = []\n",
    "    trial_type_markers = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "    trial_vals = [event_dict[key] for key in trial_type_markers]\n",
    "    n_epochs = len(trial_list)\n",
    "\n",
    "    for idx, sub_list in enumerate(trial_list):\n",
    "        # Add bad epoch if the length is not 9 (except for the last epoch):\n",
    "        if len(sub_list) != 9 and idx != n_epochs-1:\n",
    "            bad_idcs.append(idx)\n",
    "            continue\n",
    "\n",
    "        # Add bad epoch if the length is not 8 for the last epoch:\n",
    "        elif len(sub_list) != 8 and idx == n_epochs-1:\n",
    "            bad_idcs.append(idx)\n",
    "            continue\n",
    "\n",
    "        # Add bad epoch if the first entry is not a trial_type_marker:\n",
    "        if sub_list[0] not in trial_vals:\n",
    "            bad_idcs.append(idx)\n",
    "            continue\n",
    "\n",
    "        # Add bad epoch if the second entry is not a Start marker:\n",
    "        if sub_list[1] != event_dict['Start']:\n",
    "            bad_idcs.append(idx)\n",
    "            continue\n",
    "\n",
    "        # Add bad epoch if the fourth entry is not a Cue marker:\n",
    "        if sub_list[3] != event_dict['Cue']:\n",
    "            bad_idcs.append(idx)\n",
    "            continue\n",
    "\n",
    "        # Add bad epoch if the seventh entry is not a Break marker:\n",
    "        if sub_list[6] != event_dict['Break']:\n",
    "            bad_idcs.append(idx)\n",
    "            continue\n",
    "\n",
    "        # Get the keys for entries 3,5,6 and 8:\n",
    "        start_touch = list(event_dict.keys())[list(event_dict.values()).index(sub_list[2])]\n",
    "        start_release = list(event_dict.keys())[list(event_dict.values()).index(sub_list[4])]\n",
    "        target_touch = list(event_dict.keys())[list(event_dict.values()).index(sub_list[5])]\n",
    "        target_release = list(event_dict.keys())[list(event_dict.values()).index(sub_list[7])]\n",
    "\n",
    "        # Get key for the trial_type marker:\n",
    "        trial_type = list(event_dict.keys())[list(event_dict.values()).index(sub_list[0])]\n",
    "\n",
    "        # Add bad epoch if first two ldr readings are not coherent with the trial type:\n",
    "        if (trial_type[0].lower() != start_touch[2]) or (trial_type[0].lower() != start_release[2]):\n",
    "            bad_idcs.append(idx)\n",
    "            continue\n",
    "\n",
    "        # Add bad epoch if the second two ldr readings are not coherent with the second part of the trial type:\n",
    "        if (trial_type[4] == 'l'):\n",
    "            if (trial_type[2].lower() != target_touch[2]) or (trial_type[2].lower() != target_release[2]):\n",
    "                bad_idcs.append(idx)\n",
    "                continue\n",
    "\n",
    "        if (trial_type[4] == 's'):\n",
    "            if (target_touch[2] != 'c') or (target_release[2] != 'c'):\n",
    "                bad_idcs.append(idx)\n",
    "                continue\n",
    "\n",
    "    return bad_idcs\n",
    "\n",
    "def convert_samps_to_time(first_time, first_samp, samp_list):\n",
    "    \"\"\"Convert sample numbers to time values.\n",
    "    :param first_time: float time value of the first sample\n",
    "    :param first_samp: int sample number of the first sample\n",
    "    :param samp_list: list of int sample numbers to be converted\n",
    "    :return: numpy ndarray of time values for the input sample numbers\n",
    "    \"\"\"\n",
    "    return np.array(samp_list) * first_time / first_samp\n",
    "\n",
    "def create_bad_annotations(starting_times, bad_events, duration, orig_time):\n",
    "    \"\"\"Create annotations for bad events in EEG data.\n",
    "\n",
    "    :param starting_times: 1D array of starting times for all events in EEG data\n",
    "    :type starting_times: numpy.ndarray\n",
    "    :param bad_events: Indices of bad events in the starting_times array\n",
    "    :type bad_events: numpy.ndarray or list\n",
    "    :param duration: Duration of the bad events\n",
    "    :type duration: float\n",
    "    :param orig_time: The time at which the first sample in data was recorded\n",
    "    :type orig_time: float\n",
    "    :return: mne.Annotations object containing onsets, durations, and descriptions for bad events\n",
    "    :rtype: mne.Annotations\n",
    "    \"\"\"\n",
    "\n",
    "    bad_times = starting_times[bad_events]\n",
    "    onsets = bad_times + 0.01\n",
    "    durations = [duration] * len(bad_times)\n",
    "    descriptions = ['bad epoch'] * len(bad_times)\n",
    "    return mne.Annotations(onsets, durations, descriptions, orig_time=orig_time)\n",
    "\n",
    "def rename_annotations(descriptions):\n",
    "    \"\"\"\n",
    "        Rename the annotations of touch/release markers in the form of\n",
    "        new_marker = trial_type + period + position + state\n",
    "        where trial_type e.g. 'LTR-l'\n",
    "        period is either 'i' (indication) or 'c' (cue)\n",
    "        position is the position from the marker e.g. the 't' from c t 0\n",
    "        state is the touch or release state from the marker e.g. for c t 0 the state is '0' (touch). '1' would be release.\n",
    "\n",
    "        :param descriptions: list of strings, annotations to rename\n",
    "        :return: list of strings, renamed annotations\n",
    "    \"\"\"\n",
    "\n",
    "    trial_type_markers = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "    for i, entry in enumerate(descriptions):\n",
    "        if entry in trial_type_markers:\n",
    "            if 'bad' in descriptions[i+1]:\n",
    "                continue\n",
    "            else:\n",
    "                trial_type = entry\n",
    "                period = 'i' # indication\n",
    "                position = descriptions[i+2][2]\n",
    "                state = descriptions[i+2][4]\n",
    "\n",
    "                descriptions[i+2] = trial_type + '_' + period + position + state\n",
    "\n",
    "                trial_type = entry\n",
    "                period = 'i' # indication\n",
    "                position = descriptions[i+4][2]\n",
    "                state = descriptions[i+4][4]\n",
    "\n",
    "                descriptions[i+4] = trial_type + '_' + period + position + state\n",
    "\n",
    "                trial_type = entry\n",
    "                period = 'c' # cue\n",
    "                position = descriptions[i+5][2]\n",
    "                state = descriptions[i+5][4]\n",
    "\n",
    "                descriptions[i+5] = trial_type + '_' + period + position + state\n",
    "\n",
    "                trial_type = entry\n",
    "                period = 'c' # cue\n",
    "                position = descriptions[i+7][2]\n",
    "                state = descriptions[i+7][4]\n",
    "\n",
    "                descriptions[i+7] = trial_type + '_' + period + position + state\n",
    "\n",
    "    return descriptions\n",
    "\n",
    "def generate_markers_of_interest(trial_type, period, position, state):\n",
    "    \"\"\"\n",
    "    Generates markers of interest based on provided parameters\n",
    "\n",
    "    :param trial_type: A list of trial types\n",
    "    :type trial_type: list\n",
    "    :param period: A list of periods\n",
    "    :type period: list\n",
    "    :param position: A list of positions\n",
    "    :type position: list\n",
    "    :param state: A list of states\n",
    "    :type state: list\n",
    "    :return: A list of markers of interest\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    moi = []\n",
    "    for tp in trial_type:\n",
    "        for per in period:\n",
    "            for pos in position:\n",
    "                for s in state:\n",
    "                    moi.append(tp + '_' + per + pos + s)\n",
    "    return moi\n",
    "\n",
    "def replace_markers(m_stream, file):\n",
    "    fname = file[:-4] + '_cleaned.csv'\n",
    "    df_cleaned = pd.read_csv(fname)\n",
    "\n",
    "    time_series_cleaned = np.array(df_cleaned.time_series).tolist()\n",
    "    time_series_cleaned = [[mark] for mark in time_series_cleaned]\n",
    "\n",
    "    time_stamps_cleaned = np.array(df_cleaned.time_stamps)\n",
    "\n",
    "    m_stream['time_series'] = time_series_cleaned\n",
    "    m_stream['time_stamps'] = time_stamps_cleaned\n",
    "\n",
    "    return m_stream\n",
    "\n",
    "def store_scores_df(df_to_append, csv_name='classification_df.csv'):\n",
    "    # Check if dataframe exists and if not, create it:\n",
    "    if not exists(csv_name):\n",
    "        df_to_append.to_csv(csv_name)\n",
    "        return df_to_append\n",
    "    else:\n",
    "        df = pd.read_csv(csv_name, index_col=0)\n",
    "        df = pd.concat([df, df_to_append], ignore_index=True)\n",
    "        df.to_csv(csv_name)\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "def create_scores_df():\n",
    "    # Create dataframe for storing all the classification data + information:\n",
    "    cols = ['Timepoint',   # Timepoint of classification accuracy\n",
    "            'Accuracy',    # Classification accuracy\n",
    "            'Subject',     # Subject ID\n",
    "            '5-point',     # True or False based on classification approach\n",
    "            'Type',        # Cue-aligned or Movement onset-aligned\n",
    "            'Init_marker', # Marker(s) used for epoching\n",
    "            't_min',\n",
    "            't_max',\n",
    "            'epoch_info',\n",
    "            'Date',\n",
    "            'Time']\n",
    "\n",
    "    return pd.DataFrame(columns=cols)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Constants"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#data_path = 'C:/Users/tumfart/Code/github/master-thesis/data/'\n",
    "data_path = 'C:/Users/peter/Google Drive/measurements/eeg/'\n",
    "subjects = ['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07' , 'A08', 'A09', 'A10']\n",
    "# = 'A03'\n",
    "paradigm = 'paradigm' # 'eye', 'paradigm'\n",
    "plot = False\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "trial_type_markers = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "\n",
    "# Create path list for each subject:\n",
    "paths = [str(data_path + subject + '/' + paradigm) for subject in subjects]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Read xdf-files for specified subject"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Extracting subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '.xdf' in f]\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "        file = path + '/' + file_name\n",
    "\n",
    "        # Read the raw stream:\n",
    "        streams, header = pyxdf.load_xdf(file)\n",
    "\n",
    "        # Split the streams:\n",
    "        eeg_stream, marker_stream = split_streams(streams)\n",
    "\n",
    "        # Replace markers by cleaned markers if the subject is A03:\n",
    "        if subject == 'A03':\n",
    "            marker_stream = replace_markers(marker_stream, file)\n",
    "\n",
    "        # Get the eeg data:\n",
    "        eeg, eeg_ts = extract_eeg(eeg_stream, kick_last_ch=True)\n",
    "        #max_eeg_ts.append(eeg_ts.max())\n",
    "\n",
    "        # Extract all infos from the EEG stream:\n",
    "        fs, ch_names, ch_labels, eff_fs = extract_eeg_infos(eeg_stream)\n",
    "\n",
    "        # Extract the triggers from the marker stream:\n",
    "        triggers = extract_annotations(marker_stream, first_samp=eeg_ts[0])\n",
    "\n",
    "        # Define MNE annotations\n",
    "        annotations = mne.Annotations(triggers['onsets'], triggers['duration'], triggers['description'], orig_time=None)\n",
    "\n",
    "        # Create mne info:\n",
    "        # TODO: Check what info can be added to the stream:\n",
    "        info = mne.create_info(ch_names, fs, ch_labels)\n",
    "\n",
    "        # Create the raw array and add info, montage and annotations:\n",
    "        raw = mne.io.RawArray(eeg, info, first_samp=eeg_ts[0])\n",
    "        raw.set_montage('standard_1005')\n",
    "        raw.set_annotations(annotations)\n",
    "\n",
    "        # Store the raw file:\n",
    "        store_name = path + '/' + subject + '_run_' + str(i + 1) + '_unprocessed_raw.fif'\n",
    "        raw.save(store_name, overwrite=True)\n",
    "\n",
    "        if plot:\n",
    "            raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                     remove_dc=False, title='Raw')\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished reading, took me {round(time.time()-start)} seconds...')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Filter the signals"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and load the raw files:\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading raw files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_unprocessed_raw.fif' in f]\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "        if plot:\n",
    "            raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                     remove_dc=False, title='Highpass filtered')\n",
    "            plot_spectrum(raw)\n",
    "\n",
    "\n",
    "        # Highpass filter:\n",
    "        raw_highpass = raw.copy().filter(l_freq=0.4, h_freq=None, picks=['eeg'], method='iir')\n",
    "        if plot:\n",
    "            raw_highpass.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                              remove_dc=False, title='Highpass filtered')\n",
    "            plot_spectrum(raw_highpass)\n",
    "\n",
    "        # Notch filter:\n",
    "        raw_notch = raw_highpass.copy().notch_filter(freqs=[50], picks=['eeg'])\n",
    "        if plot:\n",
    "            raw_notch.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title='Notch filtered')\n",
    "            plot_spectrum(raw_notch)\n",
    "\n",
    "        # Store the raw file:\n",
    "        store_name = path + '/' + subject + '_run_' + str(i + 1) + '_highpass_notch_filtered_raw.fif'\n",
    "        raw_notch.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished highpass and notch filtering, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1. Visualize signals for bad channel identification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specifiy subject:\n",
    "subject = 'A01'\n",
    "paradigm = 'paradigm'\n",
    "if paradigm == 'paradigm':\n",
    "    runs = 9\n",
    "else:\n",
    "    runs = 2\n",
    "names = [subject + '_run_' + str(i + 1) + '_highpass_notch_filtered_raw.fif' for i in range(runs)]\n",
    "\n",
    "for i, name in enumerate(names):\n",
    "    file = data_path + subject + '/' + paradigm + '/' + name\n",
    "    raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "    raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title=f'Notch & HP filtered. Run: {i+1}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specifiy subject:\n",
    "subject = 'A10'\n",
    "paradigm = 'eye'\n",
    "run = 2\n",
    "name = subject + '_run_' + str(run) + '_highpass_notch_filtered_raw.fif'\n",
    "file = data_path + subject + '/' + paradigm + '/' + name\n",
    "\n",
    "raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title=f'Notch & HP filtered. Run: {run}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subject = 'A06'\n",
    "run = 1\n",
    "paradigm = 'paradigm'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add bad channel to bad channel.csv:\n",
    "bad_df = add_bad_channel_to_df([subject, run, paradigm, 'F7'], ch_names=raw.ch_names, csv_name='bad_channels.csv')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Add bad channels to all raw infos:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if 'raw.fif' in f]\n",
    "    # Add bad channels:\n",
    "    bads = get_bads_for_subject(subject, csv_file='bad_channels.csv')\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "        raw.info['bads'] = bads\n",
    "\n",
    "        # Overwrite the raw file with the added info:\n",
    "        store_name = path + '/' + file_name\n",
    "        raw.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished bad channel adding, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Perform interpolation of bad channels:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_highpass_notch_filtered_raw.fif' in f]\n",
    "\n",
    "    for i, filename in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + filename\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "        # Interpolate bad channels:\n",
    "        raw_interp = raw.copy().interpolate_bads(reset_bads=False)\n",
    "\n",
    "        # Overwrite the raw file with the added info:\n",
    "        store_name = path + '/' + subject + '_run_' + str(i + 1) + '_bad_channels_interpolated_raw.fif'\n",
    "        raw_interp.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished interpolating bad channels, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. CAR re-referencing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "\n",
    "    #TODO: Loaded fif file changes in final pipeline (because eye artifact correction was not yet implemented).\n",
    "    file_names = [f for f in listdir(path) if '_bad_channels_interpolated_raw.fif' in f]\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw_interp = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "        # Interpolate bad channels:\n",
    "        raw_avg_ref = raw_interp.copy().set_eeg_reference(ref_channels='average')\n",
    "\n",
    "        # Overwrite the raw file with the added info:\n",
    "        store_name = path + '/' + subject + '_run_' + str(i + 1) + '_car_referenced_raw.fif'\n",
    "        raw_avg_ref.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished rereferencing eeg, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load uninterpolated raw and interpolated raw:\n",
    "raw_avg_ref = mne.io.read_raw('C:/Users/peter/Google Drive/measurements/eeg/A01/paradigm/A01_run_1_car_referenced_raw.fif')\n",
    "raw_interp = mne.io.read_raw('C:/Users/peter/Google Drive/measurements/eeg/A01/paradigm/A01_run_1_bad_channels_interpolated_raw.fif')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_avg_ref.plot(duration=60, proj=False, n_channels=len(raw_avg_ref.ch_names), remove_dc=False, title=f'CAR referenced.')\n",
    "raw_interp.plot(duration=60, proj=False, n_channels=len(raw_interp.ch_names), remove_dc=False, title='Interpolated')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Helper cell to add info:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if 'raw.fif' in f]\n",
    "\n",
    "    # Get correct info:\n",
    "    meas_date, experimenter, proj_name, subject_info, line_freq, gender, dob, age_at_meas = get_all_additional_information(subject, csv_file='participant_info.csv')\n",
    "\n",
    "    big_subject_info = {'Subject ID': subject,\n",
    "                        'Gender': gender,\n",
    "                        'Age at measurement': age_at_meas}\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "        # Add infos:\n",
    "        raw.info['subject_info'] = big_subject_info\n",
    "        raw.info['experimenter'] = experimenter\n",
    "        #raw.info['proj_name'] = proj_name\n",
    "        raw.set_meas_date(meas_date)\n",
    "        raw.info['line_freq'] = line_freq\n",
    "\n",
    "        # Overwrite the raw file with the added info:\n",
    "        store_name = path + '/' + file_name\n",
    "        raw.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished adding info, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 11. HEAR - High-variance electrode artifact removal algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: Implement HEAR\n",
    "# Get resting data:\n",
    "\n",
    "# Check resting trials and exclude bad ones:\n",
    "\n",
    "# Calculate variance Âµ^2_s\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 13. Combine the datasets into one dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_car_referenced_raw.fif' in f]\n",
    "\n",
    "    raws = []\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'#', end=' ')\n",
    "\n",
    "        file = path + '/' + file_name\n",
    "        raw = mne.io.read_raw(file, preload=True)\n",
    "        raws.append(raw)\n",
    "\n",
    "    concat_raw = mne.concatenate_raws(raws)\n",
    "\n",
    "    # Store the concatenated raw file:\n",
    "    store_name = path + '/' + subject + '_' + paradigm + '_concatenated_raw.fif'\n",
    "    concat_raw.save(store_name, overwrite=True)\n",
    "    print()\n",
    "\n",
    "print(f'Finished concatenating, took me {round(time.time() - start)} seconds...')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "concat_raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title=f'Concatenated raw.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 12. Lowpass filter at 3 Hz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and load the raw files:\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading raw files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_concatenated_raw.fif' in f]\n",
    "\n",
    "    print(f'#', end=' ')\n",
    "\n",
    "    file = path + '/' + file_names[0]\n",
    "    raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "    # Lowpass filter:\n",
    "    raw_lowpass = raw.copy().filter(l_freq=None, h_freq=3.0, picks=['eeg'], method='iir')\n",
    "    if plot:\n",
    "        raw_lowpass.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                          remove_dc=False, title='Highpass filtered')\n",
    "        plot_spectrum(raw_lowpass)\n",
    "\n",
    "    # Store the raw file:\n",
    "    store_name = path + '/' + subject + '_lowpass_filtered_raw.fif'\n",
    "    raw_lowpass.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished lowpass filtering, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 14. Mark bad dataspans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_lowpass_filtered_raw.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "    events_from_annot, event_dict = mne.events_from_annotations(raw)\n",
    "\n",
    "\n",
    "    # Select subset of event_dict with following markers:\n",
    "    markers_of_interest = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "    event_dict_of_interest = get_subset_of_dict(event_dict, markers_of_interest)\n",
    "\n",
    "    # Check if the order of annotations is correct:\n",
    "    # Therefore first create a marker list of each trial:\n",
    "    trial_list, starting_samples = create_sliced_trial_list(event_dict, events_from_annot)\n",
    "    starting_times = convert_samps_to_time(raw.first_time, raw.first_samp, starting_samples)\n",
    "    bad_events = get_bad_epochs(event_dict, trial_list)\n",
    "    print(len(bad_events))\n",
    "\n",
    "    # add annotation for bad channels and select reject_by_annotation when generating the epochs:\n",
    "    bad_annots = create_bad_annotations(starting_times, bad_events, duration=7, orig_time=raw.info['meas_date'])\n",
    "    raw.set_annotations(raw.annotations + bad_annots)\n",
    "\n",
    "    # Rename annotations to make them unique:\n",
    "    raw.annotations.description = rename_annotations(raw.annotations.description)\n",
    "    # Save epochs:\n",
    "    store_name = path + '/' + subject + '_' + paradigm + '_bad_annotations_raw.fif'\n",
    "    raw.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f'Finished adding bad annotations, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mark bad epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specifiy subject:\n",
    "subject = 'A01'\n",
    "paradigm = 'paradigm'\n",
    "name = subject + '_' + paradigm + '_concatenated_raw.fif'\n",
    "file = data_path + subject + '/' + paradigm + '/' + name\n",
    "\n",
    "raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title=f'Concatenated eeg for subject: {subject}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bad_ids = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add bad epoch index to bad epochs csv:\n",
    "for idx in bad_ids:\n",
    "    bad_df = add_bad_epoch_to_df([subject, paradigm, idx], csv_name='bad_epochs.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 15. Epoching"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mne.set_log_level('INFO')\n",
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_bad_annotations_raw.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "    events_from_annot, event_dict = mne.events_from_annotations(raw)\n",
    "\n",
    "\n",
    "    # Select subset of event_dict with following markers:\n",
    "    epoch_type = 'movement onset-aligned 4 class direction'\n",
    "    markers_of_interest = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "\n",
    "    # Looking at cue touch:\n",
    "    trial_type = trial_type_markers\n",
    "    period = ['i'] # 'i', 'c' .. indication, cue\n",
    "    position = ['l', 'r', 't', 'b', 'c']\n",
    "    state = ['1'] # 0,1 .. touch/release\n",
    "    # markers_of_interest = generate_markers_of_interest(trial_type, period, position, state)\n",
    "\n",
    "    event_dict_of_interest = get_subset_of_dict(event_dict, markers_of_interest)\n",
    "\n",
    "    # TODO select event ID's of interest, hand over dict for event_id to make it easier to extract them:\n",
    "    epochs = mne.Epochs(raw, events_from_annot, event_id=event_dict_of_interest, tmin=0.0, tmax=7.0, baseline=None, reject_by_annotation=True, preload=True, picks=['eeg'], reject=dict(eeg=200e-6 ))\n",
    "\n",
    "    # Downsample to 10 Hz:\n",
    "    # epochs = epochs.copy().resample(10)\n",
    "\n",
    "    # Save epochs:\n",
    "    store_name = path + '/' + subject + '_' + paradigm + '_epo.fif'\n",
    "    epochs.save(store_name, overwrite=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "mne.set_log_level('WARNING')\n",
    "print(f'Finished epoching, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check signals:\n",
    "# full_epochs_short = mne.Epochs()\n",
    "first_time = True\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}')\n",
    "    file_names = [f for f in listdir(path) if 'epo.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    epochs = mne.read_epochs(file, preload=True)\n",
    "\n",
    "    epochs.info['bads'] = []\n",
    "    # Get condition:\n",
    "    longs = [m for m in markers_of_interest if '-l' in m]\n",
    "    shorts = [m for m in markers_of_interest if '-s' in m]\n",
    "    epochs_long = epochs[longs]\n",
    "    epochs_short = epochs[shorts]\n",
    "    evoked_long = epochs_long.average()\n",
    "    evoked_short = epochs_short.average()\n",
    "\n",
    "    if first_time:\n",
    "        full_epochs_long = epochs_long\n",
    "        full_epochs_short = epochs_short\n",
    "        first_time = False\n",
    "    else:\n",
    "        full_epochs_long = mne.concatenate_epochs([full_epochs_long, epochs_long])\n",
    "        full_epochs_short = mne.concatenate_epochs([full_epochs_short, epochs_short])\n",
    "\n",
    "    evokeds = dict(short=list(epochs_long.iter_evoked()),\n",
    "                    long=list(epochs_short.iter_evoked()))\n",
    "    mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=['C1'], show_sensors='upper right')\n",
    "    #\n",
    "    # times = np.arange(-2.0, 3.4, .5)\n",
    "    # evoked_long.plot_topomap(times, ch_type='eeg')\n",
    "    # evoked_short.plot_topomap(times, ch_type='eeg')\n",
    "\n",
    "evokeds = dict(short=list(full_epochs_long.iter_evoked()),\n",
    "                long=list(full_epochs_short.iter_evoked()))\n",
    "mne.viz.plot_compare_evokeds(evokeds, picks='C1', show_sensors='upper right')\n",
    "\n",
    "times = np.arange(0.0, 6.9, .5)\n",
    "full_epochs_long.average().plot_topomap(times, ch_type='eeg')\n",
    "full_epochs_short.average().plot_topomap(times, ch_type='eeg')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GLM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mne.set_log_level('INFO')\n",
    "start = time.time()\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}')\n",
    "    file_names = [f for f in listdir(path) if 'epo.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    epochs = mne.read_epochs(file, preload=True)\n",
    "\n",
    "    S = create_parameter_matrix(epochs, z_scoring=True) # matrix with shape n_conditions x n_epochs\n",
    "    epochs_fit, epochs_res, A = glm(S, epochs, shrinkage=False)\n",
    "\n",
    "    # Save regression coefficients:\n",
    "    store_name = path + '/' + subject + '_' + paradigm + 'regr_coeff.npy'\n",
    "    np.save(store_name, A)    # .npy extension is added if not given\n",
    "    # epochs.save(store_name, overwrite=True)\n",
    "\n",
    "# mne.set_log_level('WARNING')\n",
    "\n",
    "print(f'Finished calculating the GLM, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.covariance import LedoitWolf\n",
    "cov = LedoitWolf().fit(S.T)\n",
    "Css_inv = np.linalg.inv(cov.covariance_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "e_rec_fit,e_rec_res,A2 = glm(S, epochs, shrinkage=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "means, lowers, uppers = [], [], []\n",
    "confidence = .95\n",
    "for ts in range(len(epochs.times)):\n",
    "    col_of_interest = A[:,0,ts] # - (A[:,2,ts] + A[:,3,ts])/2\n",
    "    values = [np.random.choice(col_of_interest, size=len(col_of_interest), replace=True).mean() for i in range(1000)]\n",
    "    means.append(np.array(values).mean())\n",
    "    lower, upper = np.percentile(values,[100*(1-confidence)/2,100*(1-(1-confidence)/2)])\n",
    "    lowers.append(lower)\n",
    "    uppers.append(upper)\n",
    "    if ts % 100 == 0:\n",
    "        print(ts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lowers = np.array(lowers)\n",
    "uppers = np.array(uppers)\n",
    "means = np.array(means)\n",
    "plt.plot(range(len(means)), list(A[13,0,:]))# - (A[13,2,:] + A[13,3,:])/2))\n",
    "plt.fill_between(range(len(means)), list(lowers), list(uppers), alpha=.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.arange(start=-2.0, stop=3.0+1/200, step=1/200)\n",
    "# only_short = np.array(track_short) - (np.array(track_horz) + np.array(track_vert))/2\n",
    "# only_long = np.array(track_long) - (np.array(track_horz) + np.array(track_vert))/2\n",
    "# only_dist = np.array(track_long) - np.array(track_short)\n",
    "\n",
    "# values = [np.random.choice(only_short,size=len(only_short),replace=True).mean() for i in range(1000)]\n",
    "# confidence = 0.99\n",
    "# lower, upper = np.percentile(values,[100*(1-confidence)/2,100*(1-(1-confidence)/2)])\n",
    "ch_name = 'Cz'\n",
    "idx = [i for i, name in enumerate(epochs.ch_names) if name == ch_name]\n",
    "chn = idx[0]\n",
    "plt.plot(x, list(A2[chn,1,:]))\n",
    "plt.plot(x, list(-A2[chn,0,:]))\n",
    "# plt.plot(x, list(A[chn,2,:]))\n",
    "# plt.plot(x, list(-1*A[chn,3,:]))\n",
    "# plt.plot([x[0], x[-1]], [only_short.mean() + upper, only_short.mean() + upper])\n",
    "# plt.plot([x[0], x[-1]], [only_short.mean() - lower, only_short.mean() - lower])\n",
    "plt.legend(['short', 'long', 'vert', 'horz'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.covariance import ledoit_wolf\n",
    "from scipy import linalg\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "# Track one channel:\n",
    "track_short, track_long = np.empty((1,len(epochs.times))), np.empty((1,len(epochs.times)))\n",
    "track_short[:], track_long[:] = np.nan, np.nan\n",
    "\n",
    "track_short = []\n",
    "track_long = []\n",
    "track_vert = []\n",
    "track_horz = []\n",
    "track_intercept = []\n",
    "\n",
    "track_short_2 = []\n",
    "track_long_2 = []\n",
    "track_vert_2 = []\n",
    "track_horz_2 = []\n",
    "track_intercept_2 = []\n",
    "\n",
    "# Test code to train a GLM:\n",
    "# Create n_channel x n_trials matrix for each timestamp:\n",
    "X_full = epochs.get_data() # Retrieves the n_trials x n_channels x n_timestamps data\n",
    "X = X_full[:,:,0].T\n",
    "X_full_recon = np.empty(X_full.shape)\n",
    "X_full_recon_shrink = np.empty(X_full.shape)\n",
    "X_residuals = np.empty(X_full.shape)\n",
    "chn = 13\n",
    "\n",
    "\n",
    "x = S.T\n",
    "y = X_full[:,:,500]\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(x,y)\n",
    "A = np.linalg.pinv(x).dot(y)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "# x = np.hstack(x)\n",
    "# y = np.hstack(y)\n",
    "mod = sm.GLM(y, x, family=sm.families.Gaussian())\n",
    "res = mod.fit()\n",
    "print(res.summary())\n",
    "#\n",
    "# for i in range(len(epochs.times)):\n",
    "#     X = X_full[:,:,i].T\n",
    "#     pseudo_inv = np.linalg.pinv(S)\n",
    "#     A = X.dot(pseudo_inv)\n",
    "#     track_short.append(A[chn,0])\n",
    "#     track_long.append(A[chn, 1])\n",
    "#     track_vert.append(A[chn, 2])\n",
    "#     track_horz.append(A[chn, 3])\n",
    "#     track_intercept.append(A[chn, 4])\n",
    "#\n",
    "#     X_hat = A.dot(S)\n",
    "#     X_full_recon[:,:,i] = X_hat.T\n",
    "#     X_residuals[:,:,i] = X.T-X_hat.T\n",
    "#\n",
    "#     Cxs = X.dot(S.T)\n",
    "#     Css, _ = ledoit_wolf(S.T)\n",
    "#     Css_inv = linalg.inv(Css)\n",
    "#\n",
    "#     A_shrink = Cxs.dot(Css_inv)\n",
    "#\n",
    "#     track_short_2.append(A_shrink[13,0])\n",
    "#     track_long_2.append(A_shrink[13, 1])\n",
    "#     track_vert_2.append(A_shrink[13, 2])\n",
    "#     track_horz_2.append(A_shrink[13, 3])\n",
    "#     track_intercept_2.append(A_shrink[13, 4])\n",
    "#\n",
    "#     X_full_recon_shrink[:,:,i] = A_shrink.dot(S).T\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(x.T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "A - reg.coef_.T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.arange(start=-2.0, stop=3.0+1/200, step=1/200)\n",
    "only_short = np.array(track_short) - (np.array(track_horz) + np.array(track_vert))/2\n",
    "only_long = np.array(track_long) - (np.array(track_horz) + np.array(track_vert))/2\n",
    "only_dist = np.array(track_long) - np.array(track_short)\n",
    "\n",
    "values = [np.random.choice(only_short,size=len(only_short),replace=True).mean() for i in range(1000)]\n",
    "confidence = 0.99\n",
    "lower, upper = np.percentile(values,[100*(1-confidence)/2,100*(1-(1-confidence)/2)])\n",
    "\n",
    "plt.plot(x, list(only_short))\n",
    "plt.plot(x, list(only_long))\n",
    "plt.plot(x, list(only_dist))\n",
    "# plt.plot([x[0], x[-1]], [only_short.mean() + upper, only_short.mean() + upper])\n",
    "# plt.plot([x[0], x[-1]], [only_short.mean() - lower, only_short.mean() - lower])\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Cxs = X.dot(S.T)\n",
    "\n",
    "from sklearn.covariance import ledoit_wolf\n",
    "from scipy import linalg\n",
    "lw_cov_, _ = ledoit_wolf(S.T)\n",
    "lw_prec_ = linalg.inv(lw_cov_)\n",
    "\n",
    "A_shrink = Cxs.dot(lw_prec_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs_recon = mne.EpochsArray(X_full_recon_shrink, info=epochs.info, events=epochs.events, tmin=epochs.tmin, event_id=epochs.event_id, reject=epochs.reject, flat=epochs.flat, reject_tmin=epochs.reject_tmin, reject_tmax=epochs.reject_tmax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_dist_full = np.empty(epochs.get_data().shape)\n",
    "X_dir_full = np.empty(epochs.get_data().shape)\n",
    "X_diff = np.empty(epochs.get_data().shape)\n",
    "for ts in range(len(epochs.times)):\n",
    "    X_dist_full[:,:,ts] = A[:,[0,1,4],ts].dot(S[[0,1,4],:]).T\n",
    "    X_dir_full[:,:,ts] = A[:,2:5,ts].dot(S[2:5,:]).T\n",
    "    X_diff[:,:,ts] = A[:,[0,1,4],ts].dot(S[[0,1,4],:]).T - A[:,2:5,ts].dot(S[2:5,:]).T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs_dist = mne.EpochsArray(X_dist_full, info=epochs.info, events=epochs.events, tmin=epochs.tmin, event_id=epochs.event_id, reject=epochs.reject, flat=epochs.flat, reject_tmin=epochs.reject_tmin, reject_tmax=epochs.reject_tmax)\n",
    "epochs_dir = mne.EpochsArray(X_dir_full, info=epochs.info, events=epochs.events, tmin=epochs.tmin, event_id=epochs.event_id, reject=epochs.reject, flat=epochs.flat, reject_tmin=epochs.reject_tmin, reject_tmax=epochs.reject_tmax)\n",
    "epochs_diff = mne.EpochsArray(X_diff, info=epochs.info, events=epochs.events, tmin=epochs.tmin, event_id=epochs.event_id, reject=epochs.reject, flat=epochs.flat, reject_tmin=epochs.reject_tmin, reject_tmax=epochs.reject_tmax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evokeds = dict(dist=list(epochs_dist.iter_evoked()),\n",
    "               dir=list(epochs_dir.iter_evoked()))\n",
    "times = np.arange(0.0, 7.0, .2)\n",
    "fig = epochs_diff.average().plot_topomap(times, ch_type='eeg')\n",
    "fig.savefig('diff_topo_cue.svg')\n",
    "# epochs_dist.average().plot_topomap(times, ch_type='eeg')\n",
    "# epochs_dir.average().plot_topomap(times, ch_type='eeg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs_recon = mne.EpochsArray(X_full_recon, info=epochs.info, events=epochs.events, tmin=epochs.tmin, event_id=epochs.event_id, reject=epochs.reject, flat=epochs.flat, reject_tmin=epochs.reject_tmin, reject_tmax=epochs.reject_tmax)\n",
    "\n",
    "\n",
    "first_time = True\n",
    "# Get condition:\n",
    "longs = [m for m in markers_of_interest if '-l' in m]\n",
    "shorts = [m for m in markers_of_interest if '-s' in m]\n",
    "epochs_long = e_rec_res[longs]\n",
    "epochs_short = e_rec_res[shorts]\n",
    "evoked_long = epochs_long.average()\n",
    "evoked_short = epochs_short.average()\n",
    "\n",
    "if first_time:\n",
    "    full_epochs_long = epochs_long\n",
    "    full_epochs_short = epochs_short\n",
    "    first_time = False\n",
    "else:\n",
    "    full_epochs_long = mne.concatenate_epochs([full_epochs_long, epochs_long])\n",
    "    full_epochs_short = mne.concatenate_epochs([full_epochs_short, epochs_short])\n",
    "\n",
    "evokeds = dict(short=list(epochs_long.iter_evoked()),\n",
    "               long=list(epochs_short.iter_evoked()))\n",
    "mne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=['C1'], show_sensors='upper right')\n",
    "#\n",
    "# times = np.arange(-2.0, 3.4, .5)\n",
    "# evoked_long.plot_topomap(times, ch_type='eeg')\n",
    "# evoked_short.plot_topomap(times, ch_type='eeg')\n",
    "\n",
    "evokeds = dict(short=list(full_epochs_long.iter_evoked()),\n",
    "               long=list(full_epochs_short.iter_evoked()))\n",
    "mne.viz.plot_compare_evokeds(evokeds, picks='C1', show_sensors='upper right')\n",
    "\n",
    "times = np.arange(0.0, 6.9, .5)\n",
    "full_epochs_long.average().plot_topomap(times, ch_type='eeg')\n",
    "full_epochs_short.average().plot_topomap(times, ch_type='eeg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Cxs = (X-X.mean()).dot((S-S.mean()).T)\n",
    "cov_mat = np.stack((X, S.T), axis = 1)\n",
    "Cxs2 = (X.dot(S)).mean() - X.mean() * S.mean()\n",
    "X_hat = A.dot(S)\n",
    "residuals = X - X_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.arange(start=-2.0, stop=3.0+1/200, step=1/200)\n",
    "# plt.plot(x, list(np.array(track_short)-np.array(track_horz)))\n",
    "plt.plot(x, track_short)\n",
    "plt.plot(x, track_long)\n",
    "plt.plot(x, track_vert)\n",
    "plt.plot(x, track_horz)\n",
    "plt.plot(x, track_intercept)\n",
    "plt.legend(['short', 'long', 'vert', 'horz', 'intercept'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.arange(start=0, stop=7+1/200, step=1/200)\n",
    "plt.plot(x, track_short_2)\n",
    "plt.plot(x, track_long_2)\n",
    "plt.plot(x, track_vert_2)\n",
    "plt.plot(x, track_horz_2)\n",
    "plt.plot(x, track_intercept_2)\n",
    "plt.legend(['short', 'long', 'vert', 'horz', 'intercept'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 16. 2-class classification:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs.info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "df_scores = create_scores_df()\n",
    "\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}')\n",
    "    file_names = [f for f in listdir(path) if 'epo.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    epochs = mne.read_epochs(file, preload=True)\n",
    "\n",
    "    # Get condition:\n",
    "    longs = [m for m in markers_of_interest if '-l' in m]\n",
    "    shorts = [m for m in markers_of_interest if '-s' in m]\n",
    "    epochs_long = epochs[longs]\n",
    "    epochs_short = epochs[shorts]\n",
    "    # epochs_long = epochs['LTR-l', 'RTL-l', 'TTB-l', 'BTT-l']\n",
    "    # epochs_short = epochs['LTR-s', 'RTL-s', 'TTB-s', 'BTT-s']\n",
    "\n",
    "    # Create data matrix X (epochs x channels x timepoints) and label vector y (epochs x 1):\n",
    "    X = np.concatenate([epochs_long.get_data(), epochs_short.get_data()])\n",
    "    y = np.concatenate([np.zeros(len(epochs_long)), np.ones(len(epochs_short))])\n",
    "\n",
    "    clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "    acc = []\n",
    "    cv_scores = []\n",
    "    n_len = X.shape[2]\n",
    "    for tp in range(X.shape[2]):\n",
    "        x = X[:,:,tp]\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "        # clf.fit(X_train, y_train)\n",
    "        # y_pred = clf.predict(X_test)\n",
    "        # acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "        scores = cross_val_score(clf, x, y, cv=LeaveOneOut())\n",
    "        cv_scores.append(scores.mean())\n",
    "\n",
    "        # Add row to the dataframe:\n",
    "        row_to_add = {'Timepoint': tp/10, 'Accuracy': scores.mean(), 'Subject': subject, '5-point': False, 'Type': epoch_type, 'Init_marker': [markers_of_interest], 't_min': epochs.tmin, 't_max': epochs.tmax, 'epoch_info': [epochs.info], 'Date':datetime.now().strftime('%Y-%m-%d'), 'Time': datetime.now().strftime('%H:%M:%S')}\n",
    "        df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "\n",
    "        if tp != X.shape[2]-1:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}', end='\\r')\n",
    "        else:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}')\n",
    "\n",
    "    # Save cv scores to pickle:\n",
    "    cv_scores_df = pd.DataFrame(cv_scores)\n",
    "    current_time = datetime.now().strftime('%Y_%m_%d-%H%M')\n",
    "    store_name = path + '/' + subject + '_cv_scores_' + current_time + '.pkl'\n",
    "    cv_scores_df.to_pickle(store_name)\n",
    "\n",
    "# Add mean of scores as subject: Mean:\n",
    "# Add row to the dataframe:\n",
    "row_to_add = {'Timepoint': (np.arange(0, X.shape[2])/10).tolist(), 'Accuracy': df_scores.groupby('Timepoint')['Accuracy'].mean().to_list(), 'Subject': ['Mean']*n_len, '5-point': [False]*n_len, 'Type': [epoch_type]*n_len, 'Init_marker': [markers_of_interest]*n_len, 't_min': [epochs.tmin]*n_len, 't_max': [epochs.tmax]*n_len, 'epoch_info': [[epochs.info]]*n_len, 'Date':[datetime.now().strftime('%Y-%m-%d')]*n_len, 'Time': [datetime.now().strftime('%H:%M:%S')]*n_len}\n",
    "df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "# Store dataframe to full classification dataframe:\n",
    "store_scores_df(df_scores)\n",
    "\n",
    "print(f'Finished classification, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5 timestamps classifier:\n",
    "start = time.time()\n",
    "\n",
    "df_scores = create_scores_df()\n",
    "\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}')\n",
    "    file_names = [f for f in listdir(path) if 'epo.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    epochs = mne.read_epochs(file, preload=True)\n",
    "\n",
    "    # Get condition:\n",
    "    longs = [m for m in markers_of_interest if '-l' in m]\n",
    "    shorts = [m for m in markers_of_interest if '-s' in m]\n",
    "    epochs_long = epochs[longs]\n",
    "    epochs_short = epochs[shorts]\n",
    "    # epochs_long = epochs['LTR-l', 'RTL-l', 'TTB-l', 'BTT-l']\n",
    "    # epochs_short = epochs['LTR-s', 'RTL-s', 'TTB-s', 'BTT-s']\n",
    "\n",
    "    # Create data matrix X (epochs x channels x timepoints) and label vector y (epochs x 1):\n",
    "    X = np.concatenate([epochs_long.get_data(), epochs_short.get_data()])\n",
    "    y = np.concatenate([np.zeros(len(epochs_long)), np.ones(len(epochs_short))])\n",
    "\n",
    "    clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "    acc = []\n",
    "    cv_scores = []\n",
    "    n_len = X.shape[2] - 4\n",
    "    for tp in range(4,X.shape[2]):\n",
    "        x = X[:,:,tp:tp+5]\n",
    "        x = np.reshape(x, (x.shape[0], x.shape[1]*x.shape[2]))\n",
    "\n",
    "        scores = cross_val_score(clf, x, y, cv=LeaveOneOut())\n",
    "        cv_scores.append(scores.mean())\n",
    "\n",
    "        # Add row to the dataframe:\n",
    "        row_to_add = {'Timepoint': tp/10, 'Accuracy': scores.mean(), 'Subject': subject, '5-point': True, 'Type': epoch_type, 'Init_marker': [markers_of_interest], 't_min': epochs.tmin, 't_max': epochs.tmax, 'epoch_info': [epochs.info], 'Date':datetime.now().strftime('%Y-%m-%d'), 'Time': datetime.now().strftime('%H:%M:%S')}\n",
    "        df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "\n",
    "\n",
    "        if tp != X.shape[2]+4:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}', end='\\r')\n",
    "        else:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}')\n",
    "\n",
    "    # Save cv scores to pickle:\n",
    "    cv_scores_df = pd.DataFrame(cv_scores)\n",
    "    current_time = datetime.now().strftime('%Y_%m_%d-%H%M')\n",
    "    store_name = path + '/' + subject + '_cv_scores_' + current_time + '.pkl'\n",
    "    cv_scores_df.to_pickle(store_name)\n",
    "\n",
    "# Add mean of scores as subject: Mean:\n",
    "# Add row to the dataframe:\n",
    "row_to_add = {'Timepoint': (np.arange(4, X.shape[2])/10).tolist(), 'Accuracy': df_scores.groupby('Timepoint')['Accuracy'].mean().to_list(), 'Subject': ['Mean']*n_len, '5-point': [True]*n_len, 'Type': [epoch_type]*n_len, 'Init_marker': [markers_of_interest]*n_len, 't_min': [epochs.tmin]*n_len, 't_max': [epochs.tmax]*n_len, 'epoch_info': [[epochs.info]]*n_len, 'Date':[datetime.now().strftime('%Y-%m-%d')]*n_len, 'Time': [datetime.now().strftime('%H:%M:%S')]*n_len}\n",
    "df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "# Store dataframe to full classification dataframe:\n",
    "store_scores_df(df_scores)\n",
    "\n",
    "print(f'Finished classification, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('classification_df.csv', index_col=0)\n",
    "\n",
    "fig = px.line(df, x='Timepoint', y='Accuracy', title='Accuracy', color='Subject', facet_row='Type', facet_col='5-point')\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"accuracies.html\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 17. 4-class classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "df_scores = create_scores_df()\n",
    "\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}')\n",
    "    file_names = [f for f in listdir(path) if 'epo.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    epochs = mne.read_epochs(file, preload=True)\n",
    "\n",
    "    # Get condition:\n",
    "    ups = [m for m in markers_of_interest if 'BT' in m]\n",
    "    downs = [m for m in markers_of_interest if 'TT' in m]\n",
    "    lefts = [m for m in markers_of_interest if 'RT' in m]\n",
    "    rights = [m for m in markers_of_interest if 'LT' in m]\n",
    "    epochs_up = epochs[ups]\n",
    "    epochs_down = epochs[downs]\n",
    "    epochs_right = epochs[rights]\n",
    "    epochs_left = epochs[lefts]\n",
    "    # epochs_long = epochs['LTR-l', 'RTL-l', 'TTB-l', 'BTT-l']\n",
    "    # epochs_short = epochs['LTR-s', 'RTL-s', 'TTB-s', 'BTT-s']\n",
    "\n",
    "    # Create data matrix X (epochs x channels x timepoints) and label vector y (epochs x 1):\n",
    "    X = np.concatenate([epochs_up.get_data(), epochs_down.get_data(), epochs_right.get_data(), epochs_left.get_data()])\n",
    "    y = np.concatenate([np.zeros(len(epochs_up)), np.ones(len(epochs_down)), 2*np.ones(len(epochs_right)), 3*np.ones(len(epochs_left))])\n",
    "\n",
    "    clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "    acc = []\n",
    "    cv_scores = []\n",
    "    n_len = X.shape[2]\n",
    "    for tp in range(X.shape[2]):\n",
    "        x = X[:,:,tp]\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "        # clf.fit(X_train, y_train)\n",
    "        # y_pred = clf.predict(X_test)\n",
    "        # acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "        scores = cross_val_score(clf, x, y, cv=LeaveOneOut())\n",
    "        cv_scores.append(scores.mean())\n",
    "\n",
    "        # Add row to the dataframe:\n",
    "        row_to_add = {'Timepoint': tp/10, 'Accuracy': scores.mean(), 'Subject': subject, '5-point': False, 'Type': epoch_type, 'Init_marker': [markers_of_interest], 't_min': epochs.tmin, 't_max': epochs.tmax, 'epoch_info': [epochs.info], 'Date':datetime.now().strftime('%Y-%m-%d'), 'Time': datetime.now().strftime('%H:%M:%S')}\n",
    "        df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "\n",
    "        if tp != X.shape[2]-1:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}', end='\\r')\n",
    "        else:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}')\n",
    "\n",
    "    # Save cv scores to pickle:\n",
    "    cv_scores_df = pd.DataFrame(cv_scores)\n",
    "    current_time = datetime.now().strftime('%Y_%m_%d-%H%M')\n",
    "    store_name = path + '/' + subject + '_cv_scores_' + current_time + '.pkl'\n",
    "    cv_scores_df.to_pickle(store_name)\n",
    "\n",
    "# Add mean of scores as subject: Mean:\n",
    "# Add row to the dataframe:\n",
    "row_to_add = {'Timepoint': (np.arange(0, X.shape[2])/10).tolist(), 'Accuracy': df_scores.groupby('Timepoint')['Accuracy'].mean().to_list(), 'Subject': ['Mean']*n_len, '5-point': [False]*n_len, 'Type': [epoch_type]*n_len, 'Init_marker': [markers_of_interest]*n_len, 't_min': [epochs.tmin]*n_len, 't_max': [epochs.tmax]*n_len, 'epoch_info': [[epochs.info]]*n_len, 'Date':[datetime.now().strftime('%Y-%m-%d')]*n_len, 'Time': [datetime.now().strftime('%H:%M:%S')]*n_len}\n",
    "df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "# Store dataframe to full classification dataframe:\n",
    "store_scores_df(df_scores)\n",
    "\n",
    "print(f'Finished classification, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5 timestamps classifier:\n",
    "start = time.time()\n",
    "\n",
    "df_scores = create_scores_df()\n",
    "\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}')\n",
    "    file_names = [f for f in listdir(path) if 'epo.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    epochs = mne.read_epochs(file, preload=True)\n",
    "\n",
    "    # Get condition:\n",
    "    ups = [m for m in markers_of_interest if 'BT' in m]\n",
    "    downs = [m for m in markers_of_interest if 'TT' in m]\n",
    "    lefts = [m for m in markers_of_interest if 'RT' in m]\n",
    "    rights = [m for m in markers_of_interest if 'LT' in m]\n",
    "    epochs_up = epochs[ups]\n",
    "    epochs_down = epochs[downs]\n",
    "    epochs_right = epochs[rights]\n",
    "    epochs_left = epochs[lefts]\n",
    "    # epochs_long = epochs['LTR-l', 'RTL-l', 'TTB-l', 'BTT-l']\n",
    "    # epochs_short = epochs['LTR-s', 'RTL-s', 'TTB-s', 'BTT-s']\n",
    "\n",
    "    # Create data matrix X (epochs x channels x timepoints) and label vector y (epochs x 1):\n",
    "    X = np.concatenate([epochs_up.get_data(), epochs_down.get_data(), epochs_right.get_data(), epochs_left.get_data()])\n",
    "    y = np.concatenate([np.zeros(len(epochs_up)), np.ones(len(epochs_down)), 2*np.ones(len(epochs_right)), 3*np.ones(len(epochs_left))])\n",
    "\n",
    "    clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "    acc = []\n",
    "    cv_scores = []\n",
    "    n_len = X.shape[2] - 4\n",
    "    for tp in range(4,X.shape[2]):\n",
    "        x = X[:,:,tp:tp+5]\n",
    "        x = np.reshape(x, (x.shape[0], x.shape[1]*x.shape[2]))\n",
    "\n",
    "        scores = cross_val_score(clf, x, y, cv=LeaveOneOut())\n",
    "        cv_scores.append(scores.mean())\n",
    "\n",
    "        # Add row to the dataframe:\n",
    "        row_to_add = {'Timepoint': tp/10, 'Accuracy': scores.mean(), 'Subject': subject, '5-point': True, 'Type': epoch_type, 'Init_marker': [markers_of_interest], 't_min': epochs.tmin, 't_max': epochs.tmax, 'epoch_info': [epochs.info], 'Date':datetime.now().strftime('%Y-%m-%d'), 'Time': datetime.now().strftime('%H:%M:%S')}\n",
    "        df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "\n",
    "\n",
    "        if tp != X.shape[2]+4:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}', end='\\r')\n",
    "        else:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}')\n",
    "\n",
    "    # Save cv scores to pickle:\n",
    "    cv_scores_df = pd.DataFrame(cv_scores)\n",
    "    current_time = datetime.now().strftime('%Y_%m_%d-%H%M')\n",
    "    store_name = path + '/' + subject + '_cv_scores_' + current_time + '.pkl'\n",
    "    cv_scores_df.to_pickle(store_name)\n",
    "\n",
    "# Add mean of scores as subject: Mean:\n",
    "# Add row to the dataframe:\n",
    "row_to_add = {'Timepoint': (np.arange(4, X.shape[2])/10).tolist(), 'Accuracy': df_scores.groupby('Timepoint')['Accuracy'].mean().to_list(), 'Subject': ['Mean']*n_len, '5-point': [True]*n_len, 'Type': [epoch_type]*n_len, 'Init_marker': [markers_of_interest]*n_len, 't_min': [epochs.tmin]*n_len, 't_max': [epochs.tmax]*n_len, 'epoch_info': [[epochs.info]]*n_len, 'Date':[datetime.now().strftime('%Y-%m-%d')]*n_len, 'Time': [datetime.now().strftime('%H:%M:%S')]*n_len}\n",
    "df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "# Store dataframe to full classification dataframe:\n",
    "store_scores_df(df_scores)\n",
    "\n",
    "print(f'Finished classification, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "epoch_type = 'cue-aligned 4 class direction short'\n",
    "df_scores = create_scores_df()\n",
    "\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}')\n",
    "    file_names = [f for f in listdir(path) if 'epo.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    epochs = mne.read_epochs(file, preload=True)\n",
    "\n",
    "    # Get condition:\n",
    "    ups = [m for m in markers_of_interest if 'BTT-s' in m]\n",
    "    downs = [m for m in markers_of_interest if 'TTB-s' in m]\n",
    "    lefts = [m for m in markers_of_interest if 'RTL-s' in m]\n",
    "    rights = [m for m in markers_of_interest if 'LTR-s' in m]\n",
    "    epochs_up = epochs[ups]\n",
    "    epochs_down = epochs[downs]\n",
    "    epochs_right = epochs[rights]\n",
    "    epochs_left = epochs[lefts]\n",
    "    # epochs_long = epochs['LTR-l', 'RTL-l', 'TTB-l', 'BTT-l']\n",
    "    # epochs_short = epochs['LTR-s', 'RTL-s', 'TTB-s', 'BTT-s']\n",
    "\n",
    "    # Create data matrix X (epochs x channels x timepoints) and label vector y (epochs x 1):\n",
    "    X = np.concatenate([epochs_up.get_data(), epochs_down.get_data(), epochs_right.get_data(), epochs_left.get_data()])\n",
    "    y = np.concatenate([np.zeros(len(epochs_up)), np.ones(len(epochs_down)), 2*np.ones(len(epochs_right)), 3*np.ones(len(epochs_left))])\n",
    "\n",
    "    clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "    acc = []\n",
    "    cv_scores = []\n",
    "    n_len = X.shape[2]\n",
    "    for tp in range(X.shape[2]):\n",
    "        x = X[:,:,tp]\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "        # clf.fit(X_train, y_train)\n",
    "        # y_pred = clf.predict(X_test)\n",
    "        # acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "        scores = cross_val_score(clf, x, y, cv=LeaveOneOut())\n",
    "        cv_scores.append(scores.mean())\n",
    "\n",
    "        # Add row to the dataframe:\n",
    "        row_to_add = {'Timepoint': tp/10, 'Accuracy': scores.mean(), 'Subject': subject, '5-point': False, 'Type': epoch_type, 'Init_marker': [markers_of_interest], 't_min': epochs.tmin, 't_max': epochs.tmax, 'epoch_info': [epochs.info], 'Date':datetime.now().strftime('%Y-%m-%d'), 'Time': datetime.now().strftime('%H:%M:%S')}\n",
    "        df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "\n",
    "        if tp != X.shape[2]-1:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}', end='\\r')\n",
    "        else:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}')\n",
    "\n",
    "    # Save cv scores to pickle:\n",
    "    cv_scores_df = pd.DataFrame(cv_scores)\n",
    "    current_time = datetime.now().strftime('%Y_%m_%d-%H%M')\n",
    "    store_name = path + '/' + subject + '_cv_scores_' + current_time + '.pkl'\n",
    "    cv_scores_df.to_pickle(store_name)\n",
    "\n",
    "# Add mean of scores as subject: Mean:\n",
    "# Add row to the dataframe:\n",
    "row_to_add = {'Timepoint': (np.arange(0, X.shape[2])/10).tolist(), 'Accuracy': df_scores.groupby('Timepoint')['Accuracy'].mean().to_list(), 'Subject': ['Mean']*n_len, '5-point': [False]*n_len, 'Type': [epoch_type]*n_len, 'Init_marker': [markers_of_interest]*n_len, 't_min': [epochs.tmin]*n_len, 't_max': [epochs.tmax]*n_len, 'epoch_info': [[epochs.info]]*n_len, 'Date':[datetime.now().strftime('%Y-%m-%d')]*n_len, 'Time': [datetime.now().strftime('%H:%M:%S')]*n_len}\n",
    "df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "# Store dataframe to full classification dataframe:\n",
    "store_scores_df(df_scores)\n",
    "\n",
    "print(f'Finished classification, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5 timestamps classifier:\n",
    "start = time.time()\n",
    "epoch_type = 'cue-aligned 4 class direction short'\n",
    "df_scores = create_scores_df()\n",
    "\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}')\n",
    "    file_names = [f for f in listdir(path) if 'epo.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    epochs = mne.read_epochs(file, preload=True)\n",
    "\n",
    "    # Get condition:\n",
    "    ups = [m for m in markers_of_interest if 'BTT-s' in m]\n",
    "    downs = [m for m in markers_of_interest if 'TTB-s' in m]\n",
    "    lefts = [m for m in markers_of_interest if 'RTL-s' in m]\n",
    "    rights = [m for m in markers_of_interest if 'LTR-s' in m]\n",
    "    epochs_up = epochs[ups]\n",
    "    epochs_down = epochs[downs]\n",
    "    epochs_right = epochs[rights]\n",
    "    epochs_left = epochs[lefts]\n",
    "    # epochs_long = epochs['LTR-l', 'RTL-l', 'TTB-l', 'BTT-l']\n",
    "    # epochs_short = epochs['LTR-s', 'RTL-s', 'TTB-s', 'BTT-s']\n",
    "\n",
    "    # Create data matrix X (epochs x channels x timepoints) and label vector y (epochs x 1):\n",
    "    X = np.concatenate([epochs_up.get_data(), epochs_down.get_data(), epochs_right.get_data(), epochs_left.get_data()])\n",
    "    y = np.concatenate([np.zeros(len(epochs_up)), np.ones(len(epochs_down)), 2*np.ones(len(epochs_right)), 3*np.ones(len(epochs_left))])\n",
    "\n",
    "    clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "    acc = []\n",
    "    cv_scores = []\n",
    "    n_len = X.shape[2] - 4\n",
    "    for tp in range(4,X.shape[2]):\n",
    "        x = X[:,:,tp:tp+5]\n",
    "        x = np.reshape(x, (x.shape[0], x.shape[1]*x.shape[2]))\n",
    "\n",
    "        scores = cross_val_score(clf, x, y, cv=LeaveOneOut())\n",
    "        cv_scores.append(scores.mean())\n",
    "\n",
    "        # Add row to the dataframe:\n",
    "        row_to_add = {'Timepoint': tp/10, 'Accuracy': scores.mean(), 'Subject': subject, '5-point': True, 'Type': epoch_type, 'Init_marker': [markers_of_interest], 't_min': epochs.tmin, 't_max': epochs.tmax, 'epoch_info': [epochs.info], 'Date':datetime.now().strftime('%Y-%m-%d'), 'Time': datetime.now().strftime('%H:%M:%S')}\n",
    "        df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "\n",
    "\n",
    "        if tp != X.shape[2]+4:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}', end='\\r')\n",
    "        else:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}')\n",
    "\n",
    "    # Save cv scores to pickle:\n",
    "    cv_scores_df = pd.DataFrame(cv_scores)\n",
    "    current_time = datetime.now().strftime('%Y_%m_%d-%H%M')\n",
    "    store_name = path + '/' + subject + '_cv_scores_' + current_time + '.pkl'\n",
    "    cv_scores_df.to_pickle(store_name)\n",
    "\n",
    "# Add mean of scores as subject: Mean:\n",
    "# Add row to the dataframe:\n",
    "row_to_add = {'Timepoint': (np.arange(4, X.shape[2])/10).tolist(), 'Accuracy': df_scores.groupby('Timepoint')['Accuracy'].mean().to_list(), 'Subject': ['Mean']*n_len, '5-point': [True]*n_len, 'Type': [epoch_type]*n_len, 'Init_marker': [markers_of_interest]*n_len, 't_min': [epochs.tmin]*n_len, 't_max': [epochs.tmax]*n_len, 'epoch_info': [[epochs.info]]*n_len, 'Date':[datetime.now().strftime('%Y-%m-%d')]*n_len, 'Time': [datetime.now().strftime('%H:%M:%S')]*n_len}\n",
    "df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "# Store dataframe to full classification dataframe:\n",
    "store_scores_df(df_scores)\n",
    "\n",
    "print(f'Finished classification, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "epoch_type = 'cue-aligned 4 class direction long'\n",
    "df_scores = create_scores_df()\n",
    "\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}')\n",
    "    file_names = [f for f in listdir(path) if 'epo.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    epochs = mne.read_epochs(file, preload=True)\n",
    "\n",
    "    # Get condition:\n",
    "    ups = [m for m in markers_of_interest if 'BTT-l' in m]\n",
    "    downs = [m for m in markers_of_interest if 'TTB-l' in m]\n",
    "    lefts = [m for m in markers_of_interest if 'RTL-l' in m]\n",
    "    rights = [m for m in markers_of_interest if 'LTR-l' in m]\n",
    "    epochs_up = epochs[ups]\n",
    "    epochs_down = epochs[downs]\n",
    "    epochs_right = epochs[rights]\n",
    "    epochs_left = epochs[lefts]\n",
    "    # epochs_long = epochs['LTR-l', 'RTL-l', 'TTB-l', 'BTT-l']\n",
    "    # epochs_short = epochs['LTR-s', 'RTL-s', 'TTB-s', 'BTT-s']\n",
    "\n",
    "    # Create data matrix X (epochs x channels x timepoints) and label vector y (epochs x 1):\n",
    "    X = np.concatenate([epochs_up.get_data(), epochs_down.get_data(), epochs_right.get_data(), epochs_left.get_data()])\n",
    "    y = np.concatenate([np.zeros(len(epochs_up)), np.ones(len(epochs_down)), 2*np.ones(len(epochs_right)), 3*np.ones(len(epochs_left))])\n",
    "\n",
    "    clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "    acc = []\n",
    "    cv_scores = []\n",
    "    n_len = X.shape[2]\n",
    "    for tp in range(X.shape[2]):\n",
    "        x = X[:,:,tp]\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "        # clf.fit(X_train, y_train)\n",
    "        # y_pred = clf.predict(X_test)\n",
    "        # acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "        scores = cross_val_score(clf, x, y, cv=LeaveOneOut())\n",
    "        cv_scores.append(scores.mean())\n",
    "\n",
    "        # Add row to the dataframe:\n",
    "        row_to_add = {'Timepoint': tp/10, 'Accuracy': scores.mean(), 'Subject': subject, '5-point': False, 'Type': epoch_type, 'Init_marker': [markers_of_interest], 't_min': epochs.tmin, 't_max': epochs.tmax, 'epoch_info': [epochs.info], 'Date':datetime.now().strftime('%Y-%m-%d'), 'Time': datetime.now().strftime('%H:%M:%S')}\n",
    "        df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "\n",
    "        if tp != X.shape[2]-1:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}', end='\\r')\n",
    "        else:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}')\n",
    "\n",
    "    # Save cv scores to pickle:\n",
    "    cv_scores_df = pd.DataFrame(cv_scores)\n",
    "    current_time = datetime.now().strftime('%Y_%m_%d-%H%M')\n",
    "    store_name = path + '/' + subject + '_cv_scores_' + current_time + '.pkl'\n",
    "    cv_scores_df.to_pickle(store_name)\n",
    "\n",
    "# Add mean of scores as subject: Mean:\n",
    "# Add row to the dataframe:\n",
    "row_to_add = {'Timepoint': (np.arange(0, X.shape[2])/10).tolist(), 'Accuracy': df_scores.groupby('Timepoint')['Accuracy'].mean().to_list(), 'Subject': ['Mean']*n_len, '5-point': [False]*n_len, 'Type': [epoch_type]*n_len, 'Init_marker': [markers_of_interest]*n_len, 't_min': [epochs.tmin]*n_len, 't_max': [epochs.tmax]*n_len, 'epoch_info': [[epochs.info]]*n_len, 'Date':[datetime.now().strftime('%Y-%m-%d')]*n_len, 'Time': [datetime.now().strftime('%H:%M:%S')]*n_len}\n",
    "df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "# Store dataframe to full classification dataframe:\n",
    "store_scores_df(df_scores)\n",
    "\n",
    "print(f'Finished classification, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5 timestamps classifier:\n",
    "start = time.time()\n",
    "epoch_type = 'cue-aligned 4 class direction long'\n",
    "df_scores = create_scores_df()\n",
    "\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}')\n",
    "    file_names = [f for f in listdir(path) if 'epo.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    epochs = mne.read_epochs(file, preload=True)\n",
    "\n",
    "    # Get condition:\n",
    "    ups = [m for m in markers_of_interest if 'BTT-l' in m]\n",
    "    downs = [m for m in markers_of_interest if 'TTB-l' in m]\n",
    "    lefts = [m for m in markers_of_interest if 'RTL-l' in m]\n",
    "    rights = [m for m in markers_of_interest if 'LTR-l' in m]\n",
    "    epochs_up = epochs[ups]\n",
    "    epochs_down = epochs[downs]\n",
    "    epochs_right = epochs[rights]\n",
    "    epochs_left = epochs[lefts]\n",
    "    # epochs_long = epochs['LTR-l', 'RTL-l', 'TTB-l', 'BTT-l']\n",
    "    # epochs_short = epochs['LTR-s', 'RTL-s', 'TTB-s', 'BTT-s']\n",
    "\n",
    "    # Create data matrix X (epochs x channels x timepoints) and label vector y (epochs x 1):\n",
    "    X = np.concatenate([epochs_up.get_data(), epochs_down.get_data(), epochs_right.get_data(), epochs_left.get_data()])\n",
    "    y = np.concatenate([np.zeros(len(epochs_up)), np.ones(len(epochs_down)), 2*np.ones(len(epochs_right)), 3*np.ones(len(epochs_left))])\n",
    "\n",
    "    clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "    acc = []\n",
    "    cv_scores = []\n",
    "    n_len = X.shape[2] - 4\n",
    "    for tp in range(4,X.shape[2]):\n",
    "        x = X[:,:,tp:tp+5]\n",
    "        x = np.reshape(x, (x.shape[0], x.shape[1]*x.shape[2]))\n",
    "\n",
    "        scores = cross_val_score(clf, x, y, cv=LeaveOneOut())\n",
    "        cv_scores.append(scores.mean())\n",
    "\n",
    "        # Add row to the dataframe:\n",
    "        row_to_add = {'Timepoint': tp/10, 'Accuracy': scores.mean(), 'Subject': subject, '5-point': True, 'Type': epoch_type, 'Init_marker': [markers_of_interest], 't_min': epochs.tmin, 't_max': epochs.tmax, 'epoch_info': [epochs.info], 'Date':datetime.now().strftime('%Y-%m-%d'), 'Time': datetime.now().strftime('%H:%M:%S')}\n",
    "        df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "\n",
    "\n",
    "        if tp != X.shape[2]+4:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}', end='\\r')\n",
    "        else:\n",
    "            print(f'Measuring timestamp {tp+1}/{X.shape[2]}')\n",
    "\n",
    "    # Save cv scores to pickle:\n",
    "    cv_scores_df = pd.DataFrame(cv_scores)\n",
    "    current_time = datetime.now().strftime('%Y_%m_%d-%H%M')\n",
    "    store_name = path + '/' + subject + '_cv_scores_' + current_time + '.pkl'\n",
    "    cv_scores_df.to_pickle(store_name)\n",
    "\n",
    "# Add mean of scores as subject: Mean:\n",
    "# Add row to the dataframe:\n",
    "row_to_add = {'Timepoint': (np.arange(4, X.shape[2])/10).tolist(), 'Accuracy': df_scores.groupby('Timepoint')['Accuracy'].mean().to_list(), 'Subject': ['Mean']*n_len, '5-point': [True]*n_len, 'Type': [epoch_type]*n_len, 'Init_marker': [markers_of_interest]*n_len, 't_min': [epochs.tmin]*n_len, 't_max': [epochs.tmax]*n_len, 'epoch_info': [[epochs.info]]*n_len, 'Date':[datetime.now().strftime('%Y-%m-%d')]*n_len, 'Time': [datetime.now().strftime('%H:%M:%S')]*n_len}\n",
    "df_scores = pd.concat([df_scores, pd.DataFrame(row_to_add)], ignore_index=True)\n",
    "# Store dataframe to full classification dataframe:\n",
    "store_scores_df(df_scores)\n",
    "\n",
    "print(f'Finished classification, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = px.line(df_scores, x='Timepoint', y='Accuracy', title='Accuracy', color='Subject')\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Create dataframe for cv scores:\n",
    "cols = subjects\n",
    "df_subject_cv_scores = pd.DataFrame(columns=cols)\n",
    "\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading cv_scores pickle file for subject {subject}')\n",
    "    file_names = [f for f in listdir(path) if 'cv_scores' in f]\n",
    "    # Taking the latest pkl file:\n",
    "    file_names.sort(reverse=True)\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    df_cv_scores = pd.read_pickle(file)\n",
    "    df_subject_cv_scores[subject] = df_cv_scores\n",
    "\n",
    "df_subject_cv_scores['Mean'] = df_subject_cv_scores.mean(axis=1)\n",
    "\n",
    "\n",
    "print(f'Finished reading cv scores, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "temp_cols = cols.copy()\n",
    "temp_cols.append('Mean')\n",
    "\n",
    "fig = px.line(df_subject_cv_scores, x=df_subject_cv_scores.index, y=temp_cols, title='Accuracy for 2 class problem')\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "confidence = 0.95\n",
    "uppers = []\n",
    "lowers = []\n",
    "uppers_c = []\n",
    "lowers_c = []\n",
    "means = []\n",
    "for i in range(len(df_subject_cv_scores.index)):\n",
    "    x = np.asarray(df_subject_cv_scores.iloc[i, :-1])\n",
    "\n",
    "    # Confidence interval via t-distribution:\n",
    "    m = x.mean()\n",
    "    s = x.std()\n",
    "    dof = len(x)-1\n",
    "\n",
    "    t_crit = np.abs(t.ppf((1-confidence)/2,dof))\n",
    "\n",
    "    upper, lower = (m-s*t_crit/np.sqrt(len(x)), m+s*t_crit/np.sqrt(len(x)))\n",
    "    uppers_c.append(upper)\n",
    "    lowers_c.append(lower)\n",
    "\n",
    "    # Bootstrapping:\n",
    "    values = [np.random.choice(x,size=len(x),replace=True).mean() for i in range(1000)]\n",
    "    means.append(x.mean())\n",
    "    upper, lower = np.percentile(values,[100*(1-confidence)/2,100*(1-(1-confidence)/2)])\n",
    "    uppers.append(upper)\n",
    "    lowers.append(lower)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df_subject_cv_scores.index, means, color='orange')\n",
    "ax.fill_between(df_subject_cv_scores.index, lowers, uppers, color='orange', alpha=.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "append_classification_scores(scores=cv_scores, subject=subject, five_point=True, epoch_type=epoch_type, init_marker=init_marker, t_zero=t_zero)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compare scores:\n",
    "subject = 'A02'\n",
    "\n",
    "print(f'Reading cv_scores pickle file for subject {subject}')\n",
    "file_names = [f for f in listdir(data_path + '/' + subject + '/paradigm') if 'cv_scores' in f]\n",
    "\n",
    "# Create dataframe for cv scores:\n",
    "cols = file_names\n",
    "df_subject_cv_scores = pd.DataFrame(columns=cols)\n",
    "\n",
    "for f in file_names:\n",
    "    print(f'Reading cv_scores pickle file for subject {subject}')\n",
    "\n",
    "    # Load file\n",
    "    file = data_path + '/' + subject + '/paradigm/' + f\n",
    "    print(f)\n",
    "    df_cv_scores = pd.read_pickle(file)\n",
    "    df_subject_cv_scores[f] = df_cv_scores\n",
    "\n",
    "df_subject_cv_scores['Mean'] = df_subject_cv_scores.mean(axis=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pickle file:\n",
    "cv_scores_df_loaded = pd.read_pickle(store_name)\n",
    "\n",
    "\n",
    "t = np.arange(len(cv_scores_df_loaded))\n",
    "t = t/10\n",
    "#plt.plot(t, acc)\n",
    "\n",
    "plt.plot(t, cv_scores_df_loaded)\n",
    "\n",
    "window = 7\n",
    "\n",
    "# ma = np.convolve(cv_scores, np.ones(window), 'valid') / window\n",
    "#\n",
    "# plt.plot(t[:-window+1], ma)\n",
    "# plt.plot([2,2], [min(cv_scores), max(cv_scores)])\n",
    "# plt.title('Single sample approach, 180-fold CV')\n",
    "# plt.savefig('distance_acc_single.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv_scores_df = pd.DataFrame(cv_scores)\n",
    "cv_scores_df.to_pickle(store_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_pickle(store_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i,epoch in enumerate(epochs_long_short):\n",
    "    #print(epoch.shape)\n",
    "    # Deleting EOG channels:\n",
    "    epoch = np.delete(epoch, 40, 0)\n",
    "    epoch = np.delete(epoch, 21, 0)\n",
    "    epoch = np.delete(epoch, 16, 0)\n",
    "    X.append(epoch[:61,:])\n",
    "    y.append(list(epochs_long_short[i].event_id.values())[0])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(y)\n",
    "\n",
    "for i,label in enumerate(y):\n",
    "    if label % 2 == 0:\n",
    "        y[i] = 0\n",
    "    else:\n",
    "        y[i] = 1\n",
    "\n",
    "print(y)\n",
    "\n",
    "\n",
    "# Split training and test set:\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "acc = []\n",
    "cv_scores = []\n",
    "for idx in range(len(X[0,0])):\n",
    "    x = X[:,:,idx]\n",
    "    # Reshape X to 2d array:\n",
    "    #nsamples, nx, ny = x.shape\n",
    "    #x = x.reshape((nsamples,nx*ny))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "    scores = cross_val_score(clf, x, y, cv=100)\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "    if idx:\n",
    "        10 == 0:\n",
    "        print(idx)\n",
    "\n",
    "print('Done')\n",
    "\n",
    "t = np.arange(len(acc))\n",
    "t = t/10\n",
    "#plt.plot(t, acc)\n",
    "\n",
    "plt.plot(t, cv_scores)\n",
    "\n",
    "window = 7\n",
    "\n",
    "ma = np.convolve(cv_scores, np.ones(window), 'valid') / window\n",
    "\n",
    "plt.plot(t[:-window+1], ma)\n",
    "plt.plot([2,2], [min(cv_scores), max(cv_scores)])\n",
    "plt.title('Single sample approach, 180-fold CV')\n",
    "plt.savefig('distance_acc_single.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "concat_raw.info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load two files of the same kind:\n",
    "main_path = 'C:/Users/peter/Google Drive/measurements/eeg/A03'\n",
    "sub_path_1 = '/paradigm/'\n",
    "sub_path_2 = '/paradigm - Copy/'\n",
    "fname_to_compare = 'A03_paradigm_concatenated_raw.fif'\n",
    "\n",
    "f1 = main_path + sub_path_1 + fname_to_compare\n",
    "# f2 = main_path + sub_path_2 + fname_to_compare"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r1 = mne.io.read_raw(f1, preload=True)\n",
    "#r2 = mne.io.read_raw(f2, preload=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "e1 = mne.read_epochs(f1, preload=True)\n",
    "e2 = mne.read_epochs(f2, preload=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r1.info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r2.info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "e1.plot()\n",
    "e2.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "help(create_bad_annotations)\n",
    "epochs = epochs.copy().resample(10)\n",
    "epochs.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evokeds_list = [epochs['LTR-l', 'RTL-l', 'TTB-l', 'BTT-l'].average(), epochs['LTR-s', 'RTL-s', 'TTB-s', 'BTT-s'].average()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conds = ('long', 'short')\n",
    "evks = dict(zip(conds, evokeds_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "short = [m for m in markers_of_interest if '-s' in m]\n",
    "long = [m for m in markers_of_interest if '-l' in m]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evokeds2 = dict(short=list(epochs[short].iter_evoked()),\n",
    "                long=list(epochs[long].iter_evoked()))\n",
    "mne.viz.plot_compare_evokeds(evokeds2, combine=None, picks=['C1'], show_sensors='upper right')\n",
    "#picks=['Cz', 'C1', 'C2', 'FCz', 'CPz']\n",
    "# plt.savefig('distance_grand_averages.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subjects = ['A05']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mne.viz.plot_compare_evokeds(evks, picks='Fcz')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp = epochs['LTR-l', 'RTL-l']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def custom_func(x):\n",
    "    return x.max(axis=1)\n",
    "\n",
    "\n",
    "for combine in ('mean', 'median', 'gfp', custom_func):\n",
    "    mne.viz.plot_compare_evokeds(evks, picks='eeg', combine=combine)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "event_dict['BTT-s']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs['BTT-l'].plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read xdf:\n",
    "# Read the raw stream:\n",
    "streams, header = pyxdf.load_xdf('C:/Users/peter/Google Drive/measurements/eeg/A02/paradigm/sub-A02_ses-S001_task-Paradigm[_acq-]_run-002_eeg.xdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "annots = raw.annotations\n",
    "descriptions = annots.description"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i in range(1000000000):\n",
    "    pass\n",
    "print(f'Took me {time.time() - start} seconds')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trial_type = trial_type_markers\n",
    "period = ['c']\n",
    "position = ['l', 'r', 't', 'b', 'c']\n",
    "state = ['0']\n",
    "markers_of_interest = generate_markers_of_interest(trial_type, period, position, state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# subjects = ['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07', 'A08', 'A09','A10']\n",
    "subjects = ['A10']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper cell to add bad epochs to a dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: cell to view the epochs for a specific subject and marker:\n",
    "marker_of_interest = 'LTR-s' # ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "subject = 'A03'\n",
    "file = data_path + subject + '/paradigm/' + subject + '_paradigm_epo.fif'\n",
    "\n",
    "# Load epochs:\n",
    "epochs = mne.read_epochs(file, preload=True)\n",
    "\n",
    "epochs[marker_of_interest].plot()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp = epochs[marker_of_interest][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "replace_list = [list(event_dict.keys())[list(event_dict.values()).index(events_from_annot[i,2])] for i in range(len(events_from_annot))]\n",
    "\n",
    "events_from_annot[:,2] = replace_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(len(events_from_annot)):\n",
    "    events_from_annot[i,2] = list(event_dict.keys())[list(event_dict.values()).index()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get metrics of rejected channels per subject"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "\n",
    "num_bads = []\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading all fif files for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '.fif' in f]\n",
    "\n",
    "    # Load one .fif file:\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "    bads = raw.info['bads']\n",
    "    num_bads.append(len(bads))\n",
    "    print()\n",
    "\n",
    "num_bads = np.asarray(num_bads)\n",
    "print(f'Rejceted on average {num_bads.mean()} +/- {round(num_bads.std(),2)}')\n",
    "\n",
    "print(f'Finished calculating rejected channel metrics, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List files in folder:\n",
    "files = [f for f in listdir(path)]\n",
    "\n",
    "eeg_streams = []\n",
    "marker_streams = []\n",
    "# Load all recorded EEG files for one subjectc\n",
    "files = [files[0]]\n",
    "for file in files:\n",
    "    file_name = path + '/' + file\n",
    "    print(f'####', end='#')\n",
    "\n",
    "    # Read streams\n",
    "    streams, header = pyxdf.load_xdf(file_name)\n",
    "\n",
    "    # Split the streams:\n",
    "    eeg_stream, marker_stream = split_streams(streams)\n",
    "\n",
    "    eeg_streams.append(eeg_stream)\n",
    "    marker_streams.append(marker_stream)\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Finished reading, found {len(eeg_streams)} EEG streams and {len(marker_streams)} marker streams...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "differences = [0]\n",
    "max_eeg_ts = []\n",
    "for i, (eeg_stream, m_stream) in enumerate(zip(eeg_streams, marker_streams)):\n",
    "    # Get the eeg data:\n",
    "    eeg, eeg_ts = extract_eeg(eeg_stream)\n",
    "    max_eeg_ts.append(eeg_ts.max())\n",
    "\n",
    "    # Kick the last row (unused Brainproduct markers):\n",
    "    eeg = eeg[:64,:]\n",
    "\n",
    "    # Extract all infos from the EEG stream:\n",
    "    fs, ch_names, ch_labels, eff_fs = extract_eeg_infos(eeg_stream)\n",
    "\n",
    "    # Extract the markers and timestamps:\n",
    "    # markers = m_stream['time_series']\n",
    "    # markers_ts = m_stream['time_stamps']\n",
    "    #\n",
    "    # # Convert list of list of strings to list of strings:\n",
    "    # markers = [''.join(element) for element in markers]\n",
    "\n",
    "    # # Make Nan array with len(eeg)\n",
    "    # aligned_markers = np.empty(eeg_ts.shape, dtype='<U5')\n",
    "    #\n",
    "    # # Place markers string at the align array where first time markers_ts <= eeg_ts:\n",
    "    # for k, marker in enumerate(markers):\n",
    "    #     ts = markers_ts[k]\n",
    "    #     idx = np.where(ts <= eeg_ts)[0][0]\n",
    "    #     aligned_markers[idx] = marker\n",
    "\n",
    "    if i == 0:\n",
    "        global_eeg = eeg\n",
    "        first_ts = eeg_ts[0]\n",
    "        # global_markers = aligned_markers\n",
    "    else:\n",
    "        global_eeg = np.concatenate((global_eeg, eeg), axis=1)\n",
    "        # global_markers = np.concatenate((global_markers, aligned_markers))\n",
    "        differences.append(eeg_ts[0]-last_ts)\n",
    "\n",
    "    last_ts = eeg_ts[-1]\n",
    "    print(f'####', end='#')\n",
    "\n",
    "cum_diff = np.cumsum(differences)\n",
    "eeg = global_eeg\n",
    "# markers = global_markers\n",
    "print()\n",
    "print('Extracted EEG data, EEG infos...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# annotation generation from:\n",
    "# https://github.com/WriessneggerLab/EEG-preprocessing/blob/eeg/src/EEGAnalysis.py\n",
    "# generation of the events according to the definition\n",
    "triggers = {'onsets': [], 'duration': [], 'description': []}\n",
    "global_markers_ts = []\n",
    "for i, m_stream in enumerate(marker_streams):\n",
    "    # Extract the markers and timestamps:\n",
    "    markers = m_stream['time_series']\n",
    "    markers_ts = m_stream['time_stamps'] - float(m_stream['info']['created_at'][0])# - cum_diff[i]\n",
    "\n",
    "\n",
    "    global_markers_ts += list(markers_ts)\n",
    "    # read every trigger in the stream\n",
    "    for idx, marker_data in enumerate(markers):\n",
    "        # extract triggers information\n",
    "        triggers['onsets'].append(markers_ts[idx])\n",
    "        triggers['duration'].append(int(0))\n",
    "        # print(marker_data[0])\n",
    "        triggers['description'].append(marker_data[0])\n",
    "\n",
    "# define MNE annotations\n",
    "annotations = mne.Annotations(triggers['onsets'], triggers['duration'], triggers['description'], orig_time=None) #, orig_time=np.array(global_markers_ts))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mrks_list = list(markers_ts)\n",
    "a = []\n",
    "a += mrks_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Put extracted data into mne structure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: align annotations\n",
    "\n",
    "info = mne.create_info(ch_names, fs, ch_labels)\n",
    "\n",
    "raw = mne.io.RawArray(eeg, info, first_samp=first_ts)\n",
    "raw.set_montage('standard_1005')\n",
    "raw.set_annotations(annotations)\n",
    "\n",
    "if plot:\n",
    "    raw.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "             remove_dc=False, title='Raw')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter with HP at 0.4Hz and BS at 50 Hz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_highpass = raw.copy().filter(l_freq=0.4, h_freq=None, picks=['eeg'], method='iir')\n",
    "if plot:\n",
    "    raw_highpass.plot(duration=60, proj=False, n_channels=len(raw.ch_names),\n",
    "                      remove_dc=False, title='Highpass filtered')\n",
    "    plot_spectrum(raw_highpass)\n",
    "\n",
    "raw_notch = raw_highpass.copy().notch_filter(freqs=[50], picks=['eeg'])\n",
    "if plot:\n",
    "    raw_notch.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title='Notch filtered')\n",
    "    plot_spectrum(raw_notch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Interpolate bad channels:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: check function --> need to mark them first\n",
    "raw_interp = raw_notch.copy().interpolate_bads(reset_bads=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correct eye artifacts:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CAR:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_avg_ref = raw_interp.copy().set_eeg_reference(ref_channels='average')\n",
    "if plot:\n",
    "    raw_avg_ref.plot(duration=60, proj=False, n_channels=len(raw.ch_names), remove_dc=False, title='CAR Referenced')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HEAR model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LP at 3.0Hz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_lp = raw_avg_ref.copy().filter(l_freq=None, h_freq=3.0, picks=['eeg'], method='iir')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract epochs before resampling (otherwise markers may get lost) and reject bad trials:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "events = mne.find_events(raw_lp, stim_channel='Markers')\n",
    "\n",
    "epochs = mne.Epochs(raw_lp, events, event_id=classes_map, tmin=1, tmax=6, preload=True, baseline=None, reject=dict(eeg=100e-6)) #, baseline=(1,2))\n",
    "\n",
    "print(epochs)\n",
    "\n",
    "if plot:\n",
    "    epochs.plot(n_epochs=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resample to 10 Hz:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs_resampled = epochs.copy().resample(10)\n",
    "print('Preprocessing finished.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementing cue-aligned (better according to Reinmar paper)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Distance decoding:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "events = mne.find_events(raw_lp, stim_channel='Markers')\n",
    "event_dict = {'short': 1, 'long': 2, 'short': 1, 'long': 2, 'short':1, 'long':2, 'short':1, 'long':2}\n",
    "\n",
    "epochs_long_short = mne.Epochs(raw_lp, events, event_id=event_dict, tmin=1, tmax=6, preload=True, baseline=None, reject=dict(eeg=100e-6))\n",
    "\n",
    "\n",
    "\n",
    "short = epochs_long_short['short'].average()\n",
    "\n",
    "long = epochs_long_short['long'].average()\n",
    "\n",
    "#evokeds = dict(short=short, long=long)\n",
    "#mne.viz.plot_compare_evokeds(evokeds, picks='POz')\n",
    "\n",
    "evokeds2 = dict(short=list(epochs_long_short['short'].iter_evoked()),\n",
    "                long=list(epochs_long_short['long'].iter_evoked()))\n",
    "mne.viz.plot_compare_evokeds(evokeds2, combine='mean', picks=['Cz', 'C1', 'C2', 'FCz', 'CPz'], show_sensors='upper right')\n",
    "plt.savefig('distance_grand_averages.pdf')\n",
    "\n",
    "#['Pz', 'POz', 'PO3', 'PO4', 'P2', 'P1', 'P2', 'Oz', 'O1', 'O2']\n",
    "\n",
    "epochs_long_short = epochs_long_short.copy().resample(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i,epoch in enumerate(epochs_long_short):\n",
    "    #print(epoch.shape)\n",
    "    # Deleting EOG channels:\n",
    "    epoch = np.delete(epoch, 40, 0)\n",
    "    epoch = np.delete(epoch, 21, 0)\n",
    "    epoch = np.delete(epoch, 16, 0)\n",
    "    X.append(epoch[:61,:])\n",
    "    y.append(list(epochs_long_short[i].event_id.values())[0])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(y)\n",
    "\n",
    "for i,label in enumerate(y):\n",
    "    if label % 2 == 0:\n",
    "        y[i] = 0\n",
    "    else:\n",
    "        y[i] = 1\n",
    "\n",
    "print(y)\n",
    "\n",
    "\n",
    "# Split training and test set:\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "acc = []\n",
    "cv_scores = []\n",
    "for idx in range(len(X[0,0])):\n",
    "    x = X[:,:,idx]\n",
    "    # Reshape X to 2d array:\n",
    "    #nsamples, nx, ny = x.shape\n",
    "    #x = x.reshape((nsamples,nx*ny))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "    scores = cross_val_score(clf, x, y, cv=100)\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "\n",
    "print('Done')\n",
    "\n",
    "t = np.arange(len(acc))\n",
    "t = t/10\n",
    "#plt.plot(t, acc)\n",
    "\n",
    "plt.plot(t, cv_scores)\n",
    "\n",
    "window = 7\n",
    "\n",
    "ma = np.convolve(cv_scores, np.ones(window), 'valid') / window\n",
    "\n",
    "plt.plot(t[:-window+1], ma)\n",
    "plt.plot([2,2], [min(cv_scores), max(cv_scores)])\n",
    "plt.title('Single sample approach, 180-fold CV')\n",
    "plt.savefig('distance_acc_single.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5 point LDA\n",
    "X = []\n",
    "y = []\n",
    "for i,epoch in enumerate(epochs_long_short):\n",
    "    #print(epoch.shape)\n",
    "    # Deleting Marker channel:\n",
    "    # Deleting EOG channels:\n",
    "    epoch = np.delete(epoch, 40, 0)\n",
    "    epoch = np.delete(epoch, 21, 0)\n",
    "    epoch = np.delete(epoch, 16, 0)\n",
    "    X.append(epoch[:61,:])\n",
    "    y.append(list(epochs_long_short[i].event_id.values())[0])\n",
    "\n",
    "for i,label in enumerate(y):\n",
    "    if label % 2 == 0:\n",
    "        y[i] = 0\n",
    "    else:\n",
    "        y[i] = 1\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "# Split training and test set:\n",
    "\n",
    "clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "acc = []\n",
    "cv_scores = []\n",
    "for idx in range(len(X[0,0])-5):\n",
    "    x = X[:,:,idx:idx+5]\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "        print(x.shape)\n",
    "    # Reshape X to 2d array:\n",
    "    nsamples, nx, ny = x.shape\n",
    "    x = x.reshape((nsamples,nx*ny))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "    scores = cross_val_score(clf, x, y, cv=100)\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "\n",
    "\n",
    "print('Done')\n",
    "#print(acc)\n",
    "\n",
    "t = np.arange(len(acc))\n",
    "t = t/10 + 5/10\n",
    "#plt.plot(t, acc)\n",
    "\n",
    "plt.plot(t, cv_scores)\n",
    "\n",
    "window = 7\n",
    "\n",
    "ma = np.convolve(cv_scores, np.ones(window), 'valid') / window\n",
    "\n",
    "plt.plot(t[window-1:], ma)\n",
    "plt.plot([2,2], [min(cv_scores), max(cv_scores)])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Accuracy (a.u.)')\n",
    "plt.title('Windowed approach accuracies, distance 180-fold CV')\n",
    "plt.savefig('distance_acc_5point.pdf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}