{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import exists\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyxdf\n",
    "import mne\n",
    "from utils import *\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime, timezone\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "from scipy import stats\n",
    "from scipy.stats import t\n",
    "\n",
    "print('Imports done...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_epochs(path, cue_based):\n",
    "    file_names = [f for f in listdir(path) if '_bad_annotations_raw.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    raw = load_raw_file(dirpath=path, file=file_names[0])\n",
    "\n",
    "    events_from_annot, event_dict = mne.events_from_annotations(raw)\n",
    "\n",
    "    if cue_based:\n",
    "        # Select subset of event_dict with following markers:\n",
    "        markers_of_interest = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "        event_dict_of_interest = get_subset_of_dict(event_dict, markers_of_interest)\n",
    "\n",
    "        epochs = mne.Epochs(raw, events_from_annot, event_id=event_dict_of_interest, tmin=0.0, tmax=7.0, baseline=None, reject_by_annotation=True, preload=True, picks=['eeg'], reject=dict(eeg=200e-6 ))\n",
    "    else:\n",
    "        # Looking at indication release (movement onset):\n",
    "        trial_type = trial_type_markers\n",
    "        period = ['i'] # 'i', 'c' .. indication, cue\n",
    "        position = ['l', 'r', 't', 'b', 'c']\n",
    "        state = ['1'] # 0,1 .. touch/release\n",
    "        markers_of_interest = generate_markers_of_interest(trial_type, period, position, state)\n",
    "\n",
    "        event_dict_of_interest = get_subset_of_dict(event_dict, markers_of_interest)\n",
    "\n",
    "        epochs = mne.Epochs(raw, events_from_annot, event_id=event_dict_of_interest, tmin=-2.0, tmax=3.5, baseline=None, reject_by_annotation=True, preload=True, picks=['eeg'], reject=dict(eeg=200e-6 ))\n",
    "\n",
    "    # Downsample to 10 Hz:\n",
    "    epochs = epochs.copy().resample(10)\n",
    "\n",
    "    return epochs, markers_of_interest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_peak_sample(epoch_type, windowed):\n",
    "    # Get timepoint where the accuracy is max:\n",
    "    idx_peak = df[(df['Type'] == epoch_type) & (df['5-point'] == windowed) & (df['Subject'] == 'Mean')]['Accuracy'].idxmax()\n",
    "    tp_peak = df['Timepoint'][idx_peak]\n",
    "\n",
    "    # Get the peak sample:\n",
    "    if 'sfreq: 10.0' in df['epoch_info'][idx_peak]:\n",
    "        fs = 10\n",
    "    elif 'sfreq: 200.0' in df['epoch_info'][idx_peak]:\n",
    "        fs = 200\n",
    "    return int(round((tp_peak - df['t_min'][idx_peak]) * fs, 0))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#data_path = 'C:/Users/tumfart/Code/github/master-thesis/data/'\n",
    "data_path = 'C:/Users/peter/Google Drive/measurements/eeg/'\n",
    "subjects = ['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07' , 'A08', 'A09', 'A10']\n",
    "# = 'A03'\n",
    "paradigm = 'paradigm' # 'eye', 'paradigm'\n",
    "plot = False\n",
    "mne.set_log_level('WARNING')\n",
    "trial_type_markers = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "# Create path list for each subject:\n",
    "paths = [str(data_path + subject + '/' + paradigm) for subject in subjects]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load classification results:\n",
    "df = pd.read_csv('classification_df.csv', index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set conditions:\n",
    "cue_based = [True, False]\n",
    "epoch_types = ['cue aligned 4 class direction (all)', 'cue aligned 4 class direction (short)', 'cue aligned 4 class direction (long)', 'movement onset aligned 4 class direction (all)', 'movement onset aligned 4 class direction (short)', 'movement onset aligned 4 class direction (long)']\n",
    "windowed = [False, True]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mne.set_log_level('INFO')\n",
    "start = time.time()\n",
    "overall_conf = []\n",
    "# Iterate over each epoch type and each window type:\n",
    "for epoch_type in epoch_types:\n",
    "    for window in windowed:\n",
    "        peak_sample = get_peak_sample(epoch_type=epoch_type, windowed=window)\n",
    "\n",
    "        # Calculate confusion matrix for each subject for the peak sample:\n",
    "        start = time.time()\n",
    "        conf_mat = []\n",
    "        for subject, path in zip(subjects, paths):\n",
    "            if 'cue' in epoch_type:\n",
    "                epochs, markers_of_interest = load_epochs(path, cue_based=True)\n",
    "            else:\n",
    "                epochs, markers_of_interest = load_epochs(path, cue_based=False)\n",
    "            print(f'Classifying subject {subject}')\n",
    "\n",
    "            # Get condition:\n",
    "            if 'all' in epoch_type:\n",
    "                ups = [m for m in markers_of_interest if 'BT' in m]\n",
    "                downs = [m for m in markers_of_interest if 'TT' in m]\n",
    "                lefts = [m for m in markers_of_interest if 'RT' in m]\n",
    "                rights = [m for m in markers_of_interest if 'LT' in m]\n",
    "            elif 'long' in epoch_type:\n",
    "                ups = [m for m in markers_of_interest if 'BTT-l' in m]\n",
    "                downs = [m for m in markers_of_interest if 'TTB-l' in m]\n",
    "                lefts = [m for m in markers_of_interest if 'RTL-l' in m]\n",
    "                rights = [m for m in markers_of_interest if 'LTR-l' in m]\n",
    "            elif 'short' in epoch_type:\n",
    "                ups = [m for m in markers_of_interest if 'BTT-s' in m]\n",
    "                downs = [m for m in markers_of_interest if 'TTB-s' in m]\n",
    "                lefts = [m for m in markers_of_interest if 'RTL-s' in m]\n",
    "                rights = [m for m in markers_of_interest if 'LTR-s' in m]\n",
    "\n",
    "            epochs_up = epochs[ups]\n",
    "            epochs_down = epochs[downs]\n",
    "            epochs_right = epochs[rights]\n",
    "            epochs_left = epochs[lefts]\n",
    "\n",
    "            # Create data matrix X (epochs x channels x timepoints) and label vector y (epochs x 1):\n",
    "            X = np.concatenate([epochs_up.get_data(), epochs_down.get_data(), epochs_right.get_data(), epochs_left.get_data()])\n",
    "            y = np.concatenate([np.zeros(len(epochs_up)), np.ones(len(epochs_down)), 2*np.ones(len(epochs_right)), 3*np.ones(len(epochs_left))])\n",
    "\n",
    "            clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "            n_len = X.shape[2]\n",
    "\n",
    "            # Calculate only for peak_smpl:\n",
    "            x = X[:,:,peak_sample]\n",
    "\n",
    "            y_pred = cross_val_predict(clf, x, y, cv=LeaveOneOut(), n_jobs=-1)\n",
    "            conf_mat.append(confusion_matrix(y, y_pred))\n",
    "\n",
    "        overall_conf.append(conf_mat)\n",
    "\n",
    "\n",
    "\n",
    "mne.set_log_level('WARNING')\n",
    "print(f'Finished epoching, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate average conf_mat:\n",
    "cm_lst = []\n",
    "cm_avg = np.zeros((overall_conf[0][0].shape))\n",
    "for cm_subject in overall_conf:\n",
    "    for cm in cm_subject:\n",
    "        cm_avg += cm\n",
    "    cm_lst.append(cm_avg/9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for cm in cm_lst:\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm).plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "6.1e+02"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the epoch type\n",
    "epoch_type = 'cue aligned 4 class direction (all)'\n",
    "\n",
    "# Get timepoint where the accuracy is max:\n",
    "idx_peak = df[(df['Type'] == epoch_type) & (df['5-point'] == False) & (df['Subject'] == 'Mean')]['Accuracy'].idxmax()\n",
    "tp_peak = df['Timepoint'][idx_peak]\n",
    "\n",
    "# Get the peak sample:\n",
    "if 'sfreq: 10.0' in df['epoch_info'][idx_peak]:\n",
    "    fs = 10\n",
    "elif 'sfreq: 200.0' in df['epoch_info'][idx_peak]:\n",
    "    fs = 200\n",
    "peak_smpl = int(round((tp_peak - df['t_min'][idx_peak]) * fs, 0))\n",
    "\n",
    "\n",
    "# Calculate confusion matrix for each subject for the peak sample:\n",
    "start = time.time()\n",
    "conf_mat = []\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Classifying subject {subject}')\n",
    "    file_names = [f for f in listdir(path) if 'epo.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    epochs = mne.read_epochs(file, preload=True)\n",
    "\n",
    "    # Get condition:\n",
    "    ups = [m for m in markers_of_interest if 'BT' in m]\n",
    "    downs = [m for m in markers_of_interest if 'TT' in m]\n",
    "    lefts = [m for m in markers_of_interest if 'RT' in m]\n",
    "    rights = [m for m in markers_of_interest if 'LT' in m]\n",
    "    epochs_up = epochs[ups]\n",
    "    epochs_down = epochs[downs]\n",
    "    epochs_right = epochs[rights]\n",
    "    epochs_left = epochs[lefts]\n",
    "\n",
    "    # Create data matrix X (epochs x channels x timepoints) and label vector y (epochs x 1):\n",
    "    X = np.concatenate([epochs_up.get_data(), epochs_down.get_data(), epochs_right.get_data(), epochs_left.get_data()])\n",
    "    y = np.concatenate([np.zeros(len(epochs_up)), np.ones(len(epochs_down)), 2*np.ones(len(epochs_right)), 3*np.ones(len(epochs_left))])\n",
    "\n",
    "    clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "    n_len = X.shape[2]\n",
    "\n",
    "    # Calculate only for peak_smpl:\n",
    "    x = X[:,:,peak_smpl]\n",
    "\n",
    "    y_pred = cross_val_predict(clf, x, y, cv=LeaveOneOut(), n_jobs=-1)\n",
    "    conf_mat.append(confusion_matrix(y, y_pred))\n",
    "\n",
    "print(f'Finished classification, took me {round(time.time() - start)} seconds...')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get grand averages for cue aligned vs. movement onset aligned"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cue aligned:\n",
    "mne.set_log_level('INFO')\n",
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "\n",
    "avg_long = []\n",
    "avg_short = []\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_bad_annotations_raw.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "    events_from_annot, event_dict = mne.events_from_annotations(raw)\n",
    "\n",
    "\n",
    "    # Select subset of event_dict with following markers:\n",
    "    epoch_type = 'movement onset-aligned 4 class direction'\n",
    "    markers_of_interest = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "\n",
    "    # Looking at cue touch:\n",
    "    trial_type = trial_type_markers\n",
    "    period = ['i'] # 'i', 'c' .. indication, cue\n",
    "    position = ['l', 'r', 't', 'b', 'c']\n",
    "    state = ['1'] # 0,1 .. touch/release\n",
    "    # markers_of_interest = generate_markers_of_interest(trial_type, period, position, state)\n",
    "\n",
    "    event_dict_of_interest = get_subset_of_dict(event_dict, markers_of_interest)\n",
    "\n",
    "    # TODO select event ID's of interest, hand over dict for event_id to make it easier to extract them:\n",
    "    epochs = mne.Epochs(raw, events_from_annot, event_id=event_dict_of_interest, tmin=0.0, tmax=7.0, baseline=None, reject_by_annotation=True, preload=True, picks=['eeg'], reject=dict(eeg=200e-6 ))\n",
    "\n",
    "\n",
    "    # Get condition:\n",
    "    longs = [m for m in markers_of_interest if '-l' in m]\n",
    "    shorts = [m for m in markers_of_interest if '-s' in m]\n",
    "    epochs_long = epochs[longs]\n",
    "    epochs_short = epochs[shorts]\n",
    "    # Get long and short epochs data\n",
    "    epochs_long = epochs_long.get_data().mean(axis=0)\n",
    "    epochs_short = epochs_short.get_data().mean(axis=0)\n",
    "    avg_long.append(epochs_long)\n",
    "    avg_short.append(epochs_short)\n",
    "\n",
    "    print()\n",
    "\n",
    "mne.set_log_level('WARNING')\n",
    "print(f'Finished epoching, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grand_avg_long = np.zeros(avg_long[0].shape)\n",
    "grand_avg_short = np.zeros(avg_short[0].shape)\n",
    "n_tp = avg_short[0].shape[1]\n",
    "for long, short in zip(avg_long, avg_short):\n",
    "    grand_avg_long += long\n",
    "    grand_avg_short += short\n",
    "\n",
    "grand_avg_long = grand_avg_long / len(subjects)\n",
    "grand_avg_short = grand_avg_short / len(subjects)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bootstrapping for confidence interval:\n",
    "n_chan, n_ts = avg_long[0].shape\n",
    "\n",
    "uppers_long = np.zeros((n_chan, n_ts))\n",
    "lowers_long = np.zeros((n_chan, n_ts))\n",
    "\n",
    "uppers_short = np.zeros((n_chan, n_ts))\n",
    "lowers_short = np.zeros((n_chan, n_ts))\n",
    "\n",
    "confidence = .95\n",
    "n_sample = 50\n",
    "for chan in range(n_chan):\n",
    "    print(chan, end='\\r')\n",
    "    for ts in range(n_ts):\n",
    "        vals_short = []\n",
    "        vals_long = []\n",
    "        for subj in range(len(subjects)):\n",
    "            vals_short.append(avg_short[subj][chan,ts])\n",
    "            vals_long.append(avg_long[subj][chan,ts])\n",
    "\n",
    "        m = np.array(vals_short).mean()\n",
    "        s = np.array(vals_short).std()\n",
    "        dof = len(vals_short)-1\n",
    "\n",
    "        t_crit = np.abs(t.ppf((1-confidence)/2,dof))\n",
    "\n",
    "        lowers_short[chan, ts], uppers_short[chan, ts] = (m-s*t_crit/np.sqrt(len(vals_short)), m+s*t_crit/np.sqrt(len(vals_short)))\n",
    "\n",
    "        m = np.array(vals_long).mean()\n",
    "        s = np.array(vals_long).std()\n",
    "        dof = len(vals_long)-1\n",
    "\n",
    "        t_crit = np.abs(t.ppf((1-confidence)/2,dof))\n",
    "\n",
    "        lowers_long[chan, ts], uppers_long[chan, ts] = (m-s*t_crit/np.sqrt(len(vals_long)), m+s*t_crit/np.sqrt(len(vals_long)))\n",
    "        # means_short = [np.random.choice(vals_short,size=len(vals_short),replace=True).mean() for i in range(n_sample)]\n",
    "        # lowers_short[chan, ts], uppers_short[chan, ts] = np.percentile(means_short,[100*(1-confidence)/2,100*(1-(1-confidence)/2)])\n",
    "        #\n",
    "        #\n",
    "        # vals_long.append(avg_long[subj][chan,ts])\n",
    "        # means_long = [np.random.choice(vals_long,size=len(vals_long),replace=True).mean() for i in range(n_sample)]\n",
    "        # lowers_long[chan, ts], uppers_long[chan, ts] = np.percentile(means_long,[100*(1-confidence)/2,100*(1-(1-confidence)/2)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ch_name = 'Cz'\n",
    "idx = [i for i, name in enumerate(epochs.ch_names) if name == ch_name][0]\n",
    "\n",
    "x = np.arange(0,7+1/200,1/200)\n",
    "plt.plot(x,grand_avg_long[idx,:]*1e6)\n",
    "plt.fill_between(x, lowers_long[idx,:]*1e6, uppers_long[idx,:]*1e6, alpha=0.1)\n",
    "plt.plot(x,grand_avg_short[idx,:]*1e6)\n",
    "plt.fill_between(x, lowers_short[idx,:]*1e6, uppers_short[idx,:]*1e6, alpha=0.1)\n",
    "plt.plot([2, 2], [-2.5, 2.5], color='black')\n",
    "plt.legend(['Long', '95%-CI', 'Short', '95%-CI', 'Cue presentation'])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Voltage (uV)')\n",
    "plt.title(f'Distance (long vs. short) movement onset aligned on channel {ch_name}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cue aligned:\n",
    "mne.set_log_level('INFO')\n",
    "# Iterate over each subject and extract the streams\n",
    "start = time.time()\n",
    "\n",
    "avg_long = []\n",
    "avg_short = []\n",
    "for subject, path in zip(subjects, paths):\n",
    "    print(f'Reading last fif file for subject {subject}', end=' ')\n",
    "    file_names = [f for f in listdir(path) if '_bad_annotations_raw.fif' in f]\n",
    "\n",
    "    # Load file\n",
    "    file_name = file_names[0]\n",
    "    file = path + '/' + file_name\n",
    "    raw = mne.io.read_raw(file, preload=True)\n",
    "\n",
    "    events_from_annot, event_dict = mne.events_from_annotations(raw)\n",
    "\n",
    "\n",
    "    # Select subset of event_dict with following markers:\n",
    "    epoch_type = 'movement onset-aligned 4 class direction'\n",
    "    markers_of_interest = ['LTR-s', 'LTR-l','RTL-s', 'RTL-l', 'TTB-s', 'TTB-l', 'BTT-s', 'BTT-l']\n",
    "\n",
    "    # Looking at cue touch:\n",
    "    trial_type = trial_type_markers\n",
    "    period = ['i'] # 'i', 'c' .. indication, cue\n",
    "    position = ['l', 'r', 't', 'b', 'c']\n",
    "    state = ['1'] # 0,1 .. touch/release\n",
    "    markers_of_interest = generate_markers_of_interest(trial_type, period, position, state)\n",
    "\n",
    "    event_dict_of_interest = get_subset_of_dict(event_dict, markers_of_interest)\n",
    "\n",
    "    # TODO select event ID's of interest, hand over dict for event_id to make it easier to extract them:\n",
    "    epochs = mne.Epochs(raw, events_from_annot, event_id=event_dict_of_interest, tmin=-2.0, tmax=3.5, baseline=None, reject_by_annotation=True, preload=True, picks=['eeg'], reject=dict(eeg=200e-6 ))\n",
    "\n",
    "\n",
    "    # Get condition:\n",
    "    longs = [m for m in markers_of_interest if '-l' in m]\n",
    "    shorts = [m for m in markers_of_interest if '-s' in m]\n",
    "    epochs_long = epochs[longs]\n",
    "    epochs_short = epochs[shorts]\n",
    "    # Get long and short epochs data\n",
    "    epochs_long = epochs_long.get_data().mean(axis=0)\n",
    "    epochs_short = epochs_short.get_data().mean(axis=0)\n",
    "    avg_long.append(epochs_long)\n",
    "    avg_short.append(epochs_short)\n",
    "\n",
    "    print()\n",
    "\n",
    "mne.set_log_level('WARNING')\n",
    "print(f'Finished epoching, took me {round(time.time() - start)} seconds...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grand_avg_long = np.zeros(avg_long[0].shape)\n",
    "grand_avg_short = np.zeros(avg_short[0].shape)\n",
    "n_tp = avg_short[0].shape[1]\n",
    "for long, short in zip(avg_long, avg_short):\n",
    "    grand_avg_long += long\n",
    "    grand_avg_short += short\n",
    "\n",
    "grand_avg_long = grand_avg_long / len(subjects)\n",
    "grand_avg_short = grand_avg_short / len(subjects)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bootstrapping for confidence interval:\n",
    "n_chan, n_ts = avg_long[0].shape\n",
    "\n",
    "uppers_long = np.zeros((n_chan, n_ts))\n",
    "lowers_long = np.zeros((n_chan, n_ts))\n",
    "\n",
    "uppers_short = np.zeros((n_chan, n_ts))\n",
    "lowers_short = np.zeros((n_chan, n_ts))\n",
    "\n",
    "confidence = .95\n",
    "n_sample = 50\n",
    "for chan in range(n_chan):\n",
    "    print(chan, end='\\r')\n",
    "    for ts in range(n_ts):\n",
    "        vals_short = []\n",
    "        vals_long = []\n",
    "        for subj in range(len(subjects)):\n",
    "            vals_short.append(avg_short[subj][chan,ts])\n",
    "            vals_long.append(avg_long[subj][chan,ts])\n",
    "\n",
    "        m = np.array(vals_short).mean()\n",
    "        s = np.array(vals_short).std()\n",
    "        dof = len(vals_short)-1\n",
    "\n",
    "        t_crit = np.abs(t.ppf((1-confidence)/2,dof))\n",
    "\n",
    "        lowers_short[chan, ts], uppers_short[chan, ts] = (m-s*t_crit/np.sqrt(len(vals_short)), m+s*t_crit/np.sqrt(len(vals_short)))\n",
    "\n",
    "        m = np.array(vals_long).mean()\n",
    "        s = np.array(vals_long).std()\n",
    "        dof = len(vals_long)-1\n",
    "\n",
    "        t_crit = np.abs(t.ppf((1-confidence)/2,dof))\n",
    "\n",
    "        lowers_long[chan, ts], uppers_long[chan, ts] = (m-s*t_crit/np.sqrt(len(vals_long)), m+s*t_crit/np.sqrt(len(vals_long)))\n",
    "        # means_short = [np.random.choice(vals_short,size=len(vals_short),replace=True).mean() for i in range(n_sample)]\n",
    "        # lowers_short[chan, ts], uppers_short[chan, ts] = np.percentile(means_short,[100*(1-confidence)/2,100*(1-(1-confidence)/2)])\n",
    "        #\n",
    "        #\n",
    "        # vals_long.append(avg_long[subj][chan,ts])\n",
    "        # means_long = [np.random.choice(vals_long,size=len(vals_long),replace=True).mean() for i in range(n_sample)]\n",
    "        # lowers_long[chan, ts], uppers_long[chan, ts] = np.percentile(means_long,[100*(1-confidence)/2,100*(1-(1-confidence)/2)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ch_name = 'C1'\n",
    "idx = [i for i, name in enumerate(epochs.ch_names) if name == ch_name][0]\n",
    "\n",
    "x = np.arange(-2.0,3.5+1/200,1/200)\n",
    "plt.plot(x,grand_avg_long[idx,:]*1e6)\n",
    "plt.fill_between(x, lowers_long[idx,:]*1e6, uppers_long[idx,:]*1e6, alpha=0.1)\n",
    "plt.plot(x,grand_avg_short[idx,:]*1e6)\n",
    "plt.fill_between(x, lowers_short[idx,:]*1e6, uppers_short[idx,:]*1e6, alpha=0.1)\n",
    "plt.plot([0, 0], [-2.5, 2.5], color='black')\n",
    "plt.legend(['Long', '95%-CI', 'Short', '95%-CI', 'Movement onset'])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Voltage (uV)')\n",
    "plt.title(f'Distance (long vs. short) movement onset aligned on channel {ch_name}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}